\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{setspace} % Add the setspace package
\usepackage{titlesec} % Add the titlesec package for customizing titles

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage[font=small]{caption}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
  breakable,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}[1]{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=#1,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
  breakable,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}[1]{
  colback=green!5!white,    % Light green background
  colframe=green!75!black,   % Darker green frame
  colbacktitle=green!85!black,  % Even darker green for the title background
  title=#1,         % Title text for example box
  fonttitle=\bfseries,    % Bold title font
  enhanced,
  breakable,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\onehalfspacing

\geometry{top=1.5in} % Adjust the value as needed

% Customization for chapter titles
\titleformat{\chapter}[display] % Use 'display' to put number and title on separate lines
  {\normalfont\LARGE\bfseries} % Format for the chapter title
  {Chapter \thechapter} % Display "Chapter X"
  {0.5em} % Space between "Chapter X" and the title
  {\Huge} % Chapter title format
\titlespacing*{\chapter}{0pt}{-20pt}{20pt} % Adjust spacing around chapter title

% ----------------------------------------------------------------



\begin{document}

\begin{titlepage}
  \centering
  {\LARGE\bfseries MPhil DIS Project 24\par}
  {\LARGE Learning Dominant Physical Processes with Data-Driven Balance Models\par}
  \vspace{1cm}
  {\includegraphics[width=0.2\textwidth]{University_Crest.pdf}\par}
  {\Large CRSiD:\ tmb76\par}
  \vspace{1cm}
  {\Large Department of Physics\par}
  {\Large\bfseries University of Cambridge\par}
  \vfill
  {\itshape Submitted in partial fulfilment of the requirements of the MPhil degree in Data Intensive Science}
  \vfill
  {\large Hughes Hall  \hspace{6cm} \today\par}
\end{titlepage}

\tableofcontents

\chapter{Executive Summary}


\chapter{Introduction}


One of the key steps of scientific method is reproducibility. Results must be reproducible by others, ensuring that the same conclusions can be drawn multiple times. If a result cannot be reproduced, it may be considered erroneous, or simply a random occurence. This project therefore has for a core aim to evaluate the reproducibility of the results of the Callaham et al. (2021)\cite{callaham2021learning} paper and to test the robustness of their method.

\vspace{5mm}

For many problems in engineering and physical sciences, equations involve a large number of terms and complex differential equations. Simulating them can be computationally expensive or unnecessarily so, due to multiple asymptotic local behaviours where the system is dominated by a subset of the terms. In such cases, one can simplify the equations to a balance between these dominant terms, and simulate the system with sufficient accuracy and relatively lower computational cost \cite{charney1990scale}. This method, known as dominant balance or scale analysis, has been a powerful tool in physics.

\vspace{5mm}

And though extremely useful, dominant balance also requires expertise and is usually done by hand in time-consuming proofs. This report discusses and verifies a novel approach, developped by Callaham et al. (2021)\cite{callaham2021learning}, which explores using data from a physical system and machine learning methods to identify dominant balances algorithmically. First, this report will focus on the paper and the research surrounding it. Delving into what the rationale behind the method is, and how it performed on a series of case studies, as well as verifying it through reproducing the results with alternative code. This will be done primarily focusing on one of the case studies, but also for most of the others. Additionally, other algorithms than the method's chosen one are used to test the robustness of the method. Second, the method will be used on a new dataset, from simulations of elasto-inertial turbulence, a property of polymer laden flow.


\chapter{Background}


As aforementionned, dominant balance or scale analysis is a powerful tool in simplifying the modelling of physical processes. Importantly, it helps better understand the physics at play in a system by identifying the subset of terms that truly matter in an equation for a given asymptotic case. Simplifying the equations also leads to easier computations by avoiding unnecessary complications of the model. Taking the example of meteorology, where modelling the entire atmosphere from the full Navier-Stokes equations of motion would have an immense computational cost. And a large part of improvements in numerical weather predictions can be attributed to scale analysis \cite{charney1947dynamics, phillips1963geostrophic, burger1958scale, yano2009scale}.

\vspace{5mm}

However, this process can be slow as it requires considerable expertise from researchers. And for most of the well studied physical systems, this was done by hand a few decades ago. But with the wealth of computational power and data science techniques there is today, an attempt at automating dominant balance can be made. First is the Portwood et al. (2016)\cite{portwood2016robust} paper which used a cumulative distribution function on local density gradients to separate each region of a stratified turbulent flow. But the method used is highly tailored for its case study, as the gradient of one of the terms is used, knowing it has dynamics discerning qualities. Further, the results are interpreted through expert analysis of the identified regions. Second is the work carried out in Lee \& Zaki (2018)\cite{lee2018detection} where an algorithm to detect different dynamical regions is introduced. But again it is through the use of case-specific variables (vorticity), which restrict the use of this algorithm to certain flows. Finally, Sonnewald et al. (2019)\cite{sonnewald2019unsupervised} used a K-Means clustering algorithm to identify dynamically distinct regions in the ocean. And though they do introduce the concept of using the terms in the governing equations as features, the identification of active terms is done through comparison of the magnitudes of each term in the equation. In other words, identifciation of dominant terms is not done algorithmically but ``manually''. Thus, these methods are mostly designed for specific case studies and partly rely on expert knowledge to interpret the results.

\vspace{5mm}

A similar challenge in data science and machine learning has been to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009)\cite{schmidt2009distilling} contributed to a breakthrough using symbolic regression to find linear and non-linear differential equations. And this was improved in Brunton et al. (2016)\cite{brunton2016discovering}. As symbolic regression is expensive, the problem was approached with sparse regression, which for high-dimensional problems means identifying a sparse governing equation. The rationale behind this is that governing equations usually having only a subset of terms being important, as in dominant balance. Lejarza \& Baldea (2022) further advanced this by using multiple basis functions and a non-linear moving horizon optimization to learn governing equations from noisy data. Deep learning methods have also been used in this effort. First, where the lagrangians are learned, therefore learning how to model complex physical systems, and learning symmetries and conservation laws, where other networks failed \cite{cranmer2020discovering}. Second, deep learning (Graph Neural Network) and symbolic regression are combined to create a general framework to recover equations of physical systems \cite{cranmer2020lagrangian}. This method has the advantage of being generalisable and therefore useable to extract plausible governing equations for unknown systems.

\vspace{5mm}

This generalisable quality is precisely the gap that Callaham et al. (2021)\cite{callaham2021learning} attempt to fill in the identification of dominant balance models. They propose a novel approach to take in simulated or measured data from a physical system, and extract dominant balance models with minimal user input. This means one could use it in conjunction with the above governing equation identifying methods, and essentially automatically learn the governing equations and asymptotic regimes of that equation for any given physical system. This is a very powerful result however, as Schmidt \& Lipson (2009) noted for their work, this method should be seen as a guiding tool to help indicate where scientists should focus their attention, rather than a definitive answer. This is an important point which will be further discussed in this report.


\chapter{Methodology}

The method proposed by Callaham et al. (2021)\cite{callaham2021learning} can be summarized in three steps. First, representing the data in an equation space, with the terms as features. Second, using the Gaussian Mixture Model clustering algorithm to identify groups with a similar balance of terms. Finally, using Sparse Principal Component Analysis to identify the active terms in each cluster. This section will delve into the details of each of these steps, with a summary flow chart in Figure \ref{fig:method_flowchart}.

\section{Data \& Equation Space Reprensentation}

As previously mentionned, obtaining the data can be done through simulations or measurements, with almost no restrictions as to what equations can be studied. The data typically comes in the form of space-time fields of physical variables: $u(\vec{x}, t)$, where $u$ is the physical variable of interest (e.g. $\vec{u}$, $p$, etc. for the Navier-Stokes equations). It is essential that one can compute all the terms of the governing equation from the variables. Additionally, the computed terms must all balance out to 0 for each point in time and space. This depends on the computation of the terms to be as they appear in the equation, which ensures a linear covariance structure, as each term will be balanced by a linear combination of the other terms\cite[Supplementary Information]{callaham2021learning}. For example, if a term involves a squared combination of variables, it must be computed in that form, and not as the combination of variables.

\vspace{5mm}

The fundamental idea of the Callaham et al. (2021) method is to take these fields of terms in the physical-space and organize them into an equation-space, where each dimension is one of the terms in the equation, and each sample is a point in space and time. Thus, each sample will be a vector $\vec{f} \in \mathbb{R}^k$ where $k$ is the number of terms in the equation, such that:

\begin{equation}
    \vec{f} = \begin{bmatrix} f_1(u(\vec{x}, t), \hdots) \\ f_2(u(\vec{x}, t), \hdots) \\ \vdots \\ f_k(u(\vec{x}, t), \hdots) \end{bmatrix}
\end{equation}

Where $f_i$ is the $i^{th}$ term in the equation, itself a function of the physical variables. Again, one must ensure that for each sample, the terms all balance out to a residual at least several orders of magnitude smaller than the terms themselves. This is in order to make sure that the equation studied or data used is not invalid.


\section{Gaussian Mixture Model Clustering}

By clustering the points in equation space, groups of points that have a similar balance of terms can be identified. Here, the chosen algorithm is the Gaussian Mixture Model (GMM) clustering algorithm. This algorithm relies on the assumption that the data has been generated from a mixture of a finite number of Gaussian distributions with unknown parameters\cite{mit2015algorithmic}. It has the advantage of being able to identify clusters with varying shapes and sizes. It only requires one hyperparameter, which is the number of clusters the algorithm must find, and therefore how many Gaussian distributions the data is assumed to have been generated from\cite{sklearnGMM}. This number is not fixed and must be chosen for each case study. Generally, the choice should be conservative, aiming for a number of clusters that is likely to be greater than the true/expected number of dominant balance regimes in the system. This is because if the number is higher than it needs to be, it will likely be the case that some of the clusters actually have identical dominant balances, and these will be combined.

\vspace{5mm}

The way GMMs fit to the data is by using the Expectation-Maximisation algorithm. This algorithm starts from an initial guess for the parameters of the gaussian distributions: $w_0$, the weight, $\mathbf{\mu}_{0}$, the vector mean of size $[1 \times n_{features}]$, and $\mathbf{\Sigma}_{0}$, the $[n_{features} \times n_{features}]$ covariance matrix, for each Gaussian. The expectation step evaluates the likelihood of a given point belonging to each cluster, for the current parameters. And the maximisation step updates the parameters with weighted averages of those posterior probabilities\cite{dempster1977maximum} (see Alg. 1, in section \ref{alg:GMM}). This converges to the maximum likelihood estimates of the parameters (see Fig. \ref{fig:GMM_example}). Once fitted, cluster membership of new data points can be determined by taking the cluster with the highest probability at that point.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{./GMMexample 1.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 2.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 3.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 4.png}
  \caption{Example of a Gaussian Mixture Model fit to data. The data is generated from 3 Gaussian distributions, and the GMM algorithm fits 3 Gaussians to it. \cite[1D Example Notebook]{gmm_towardsdatascience}}
  \label{fig:GMM_example}
\end{figure}

Another key output of the GMM is in its probabilistic nature as each Gaussian is defined by a covariance matrix of the equation's terms. This gives insight into which terms could be active in each cluster, and helps illustrate the intuition behind identifying active terms geometrically with this method. If only a subset of the terms in a cluster have non-zero covariance, then it means that it is only along that subset of dimensions that the cluster varies, and hence that the terms are non-negligible.

\section{Sparse Principal Component Analysis}

With the points grouped in clusters of similar balance of terms, the next step is to identify which terms are active and which can be dropped to simplify the equation for that cluster. This could be done by applying a theshold to the covariance matrices obtained from the GMM clustering. But it is done more robustly through Sparse Principal Component Analysis (SPCA). SPCA is a variant of Principal Component Analysis (PCA) which is a method usually used to reduce the dimensionality of the data, whilst maximising the information retained. The new dimensions onto which the data is projected are called principal components and are the directions in which the data has the greatest variance\cite{lever2017principal} (see Fig. \ref{fig:PCA_wiki}). SPCA differs from PCA in that it adds a sparsity constraint to the principal components, adding a $\ell$-1 penalty to the objective function of PCA for the number of non-zero coefficients. This then leads to some coefficients being shrunk down to 0 when the $\ell$-1 penalty factor ($\alpha$) is large enough\cite{zou2006sparse} (see Alg. 2, in section \ref{alg:SPCA}). When setting a low value for $\alpha$, SPCA will be more lenient in judging if a term is active in a cluster. The higher the value, the sparser the principal components will be. Too low, and all clusters will have a full balance, with all terms active. Too high, and all clusters will have an empty balance, with no terms active.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{./pca_wiki.png}
  \caption{Example of a 2D dataset projected onto its first 2 principal components (black arrows). The first principal component is the direction of greatest variance, and the second is orthogonal to it. \cite{pca_wikipedia}}
  \label{fig:PCA_wiki}
\end{figure}

Taking the GMM clustered data, the SPCA algorithm is applied to each cluster, and only the leading component is extracted. By taking the leading component, the algorithm is identifying the one direction along which there is the most variance within that cluster. This will indicate which terms vary the most within that cluster, and therefore which ones matter more. Then applying the sparsity constraint, the vecotr obtained will have some of its coefficients shrunk to 0, and the non-zero coefficients will indicate the active terms in that cluster.

\vspace{5mm}

To help judge what value of $\alpha$ to pick, Callaham et al. (2021) propose to apply SPCA for a range of $\alpha$ values, and computing a residual error defined as the $\ell$-2 norm of the neglected terms across all clusters. This illustrates the trade-off between sparsity (simplifying the model) and retaining information (keeping the model accurate), helping to pick an optimal value. Once SPCA is applied, each cluster is then associated to a sparse binary vector defining the active terms (1 if active, 0 if not). The balance models are then checked for uniqueness, and if some clusters have the same balance, they are combined.


\chapter{Conducted research}

To test and validate this method, Callaham et al.\ used a series of case studies\cite{callaham2021learning}. Some were of well-studied physical systems with well-known dominant balance regimes (e.g., turbulent boundary layer, geostrophic balance currents). But others were more complex and less understood (e.g., optical pulse generation, rotating detonation engine). For these latter cases, the paper presents a first plausible identification of dominant balance regimes, showing an exciting  In this chapter, the aim is to discuss the reproducibility of the results presented in the paper, as well as to test and discuss the limitations of the method.

\section{Portability of the code}

One great quality of Callaham et al. (2021)\cite{callaham2021learning} is the sharing of their code in the form of runnable notebooks. This is tied to the motivation for this project being the importance of reproducibility in today’s code-rich research environment. It is key that scientists share their code so one may verify how the results were obtained. This is also a great asset as it allows for a more thorough understanding and test of the method, as it is possible to write explicitely different code.

Overall, the code is of great quality, though some portability issues were encountered. Some of the dependencies were not stated in their README. More importantly, it seemed some of the data-generating code was modified between running their final notebook versions and their uploading to the repository. This led to some troubleshooting to get the data to match the one used in the paper. For the flow past a cylinder case, a file needed in the setup of the Nek5000 simulation software was missing\cite{nek5000setup}. For the bursting neuron case, the times for which the data was generated was wrongly set. Similarly, for the geostrophic current data, the remote reading code was selecting snapshots of the data that did not match the ones used in the paper, and the first 45 were set to zero, which had no real use. Further, the results in the paper are actually obtained from a different snapshot than the one stated in the paper. For this report, the data was simply downloaded from the HYCOM database \cite{hycom}, and the time was set to the same as the one used in the paper.

Nevertheless, the code was otherwise easy to run, and the notebook format is very useful to better the logic of the code. From this notebook, the code was first written for the turbulent boundary layer case. Explicitly alternative code was then written to test the reproducibility of the results.

\section{The turbulent boundary layer case}

Reiterating previously made points, the objective is not only to check that the results can be reproduced but to do so using alternative code. The aim here is to have code that, in practice, performs the same functions, but written differently. This is to ensure that none of the main results from Callaham et al. are due to a “lucky” bug in their code, and were obtained through random chance. Because the underlying method is essentially the same for all case studies, the choice was made to put particular focus on just one of the cases, and less so on the others. The scenario chosen was the boundary layer in transition to turbulence.

\subsection{Reproducing the code}

Throughout the Callaham et al. (2021)\cite{callaham2021learning} notebooks, aside from the three main steps of the method, the large majority of the code involves handling the data, switching from equation space to physical space, and obtaining the unique balance models. Most of it is written using \texttt{Numpy}, which is arguably a very good choice for efficient and trustworthy numerical operations. In the alternative code, however, \texttt{Pandas} was primarily used. In terms of performance, it is close to \texttt{Numpy}, especially for the size of the data in this study, and provides similar practical functions. For some code sections, however, impractical workarounds were required, particularly to replace the \texttt{numpy.unique} method, when finding and combining duplicate balance models.

The data for this case study is obtained from the John Hopkins database, which conducted a direct numerical simulation of a boundary layer over a stationary plate\cite{jhtdb}. This database provides the $x$ and $y$ coordinates as well as the following variables: $\overline{u}$, the streamwise component of velocity;  $\overline{v}$, the wall-normal component of velocity;  $\overline{p}$, the fluid pressure; and $(\overline{u{\prime} v{\prime}})$ and $(\overline{u{\prime}^2})$, the wall-normal and streamwise Reynolds stress terms, respectively. From these variables, the terms of the streamwise component of the Reynolds-Averaged Navier-Stokes (RANS) equations need to be calculated:

\begin{equation}
  \bar{u} \bar{u}_x + \bar{v} \bar{u}_y = \rho^{-1} \bar{p}_x + \nu \nabla^2 \bar{u}  - (\overline{u' v'})_y - (\overline{u'^2})_x
\end{equation}

To compute the derivative terms, the method employed by Callaham et al. uses \texttt{scipy.sparse}’s sparse matrices library. The aim is to build a derivative operator sparse matrix so that when computing the matrix product of that matrix with the fields or variables, the second-order forward/central/backward difference derivative is obtained, following the method described in ‘Fundamentals of Numerical Computations’\cite{finitediff}. Though it has the advantage of being portable for all variables, it is much slower than using the \texttt{numpy.gradient} function, which performs the same computation more explicitly when setting the \texttt{edge\_order} argument to 2. With the derivatives computed, spatial fields of each term in the equation can be plotted (see Fig. \ref{fig:RANS_terms}).

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using the original Callaham et al. code}
  \label{fig:RANS_terms}
\end{figure}

The next step is to use a Gaussian Mixture Model (GMM) to cluster the data in equation space. Callaham et al. judiciously chose to use the \texttt{sklearn} library’s implementation of the GMM algorithm. This implementation is likely very efficient, being written and optimized by experienced developers. With the chosen arguments, the algorithm is initialized using the \texttt{‘k-means++’} method. This method initializes the means of the Gaussians using the centroids found by the K-Means clustering algorithm (see Algorithm 3), which is itself initialized based on the empirical probability distributions of the data\cite{arthur2007kmeans}. The covariance matrices are then initialized as the covariance matrices of the clusters found by the K-Means algorithm. The algorithm then uses the Expectation-Maximisation algorithm as expected. Convergence is evaluated using the log-probability of the data:

\begin{equation}
  \log \mathcal{L}(\vec{X} | \vec{\theta}) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} w_k \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \vec{\Sigma}_k) \right)
\end{equation}

The algorithm stops when the difference between the new and old log-likelihood is less than $10^{-3}$.

The alternative code was thus written aiming to follow the same initialization, explicitly using \texttt{sklearn}’s K-Means algorithm with \texttt{‘k-means++’} initialization. The Expectation-Maximization algorithm is then implemented as described in Algorithm 1, using the total log-likelihood difference as the convergence criterion.

For Sparse Principal Component Analysis (SPCA), the \texttt{sklearn} library’s implementation is used. The method employed is based on an extension of sparse PCA, which utilizes structured regularization to constrain the sparsity patterns. This is also done for the alternative code, as the \texttt{sklearn} implementation employs high level methods which were hard to reproduce\cite{jenatton2010structured,mairal2010online}. However, it was decided to use a parallelising package to speed up the computation of the SPCA residuals. Here, the \texttt{joblib} package was used, providing convenient pipelining methods in Python\cite{joblib}.

With SPCA completed, the active terms can be identified. From this point, the original and alternative code only differ in which library they predominantly use. The original code primarily uses \texttt{Numpy}, while the alternative code uses \texttt{Pandas}. Alternative code was thus also written in handling the balance model results, getting the unique ones, and plotting them in space.


\subsection{Results}

First, checking the computation of the RANS equation's terms, the \texttt{scipy.sparse} method's obtained terms are plotted in Fig. \ref{fig:RANS_terms}. The alternative code, which used \texttt{numpy.gradient} instead, gave the same results (see Fig. \ref{fig:custom_RANS_terms}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/custom_RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using an alternative code}
  \label{fig:custom_RANS_terms}
\end{figure}

The next step involves clustering the data in equation space. A clear way of comparing results is to examine the obtained covariance matrices. In Figure \ref{fig:GMM_cov_mat}, the covariance matrices obtained when using \texttt{sklearn}’s Gaussian Mixture Model (GMM) routine, as well as when using the custom GMM implementation are shown. It can already be seen that some clusters clearly have the more important terms. This is the case within each method (Cluster 3 and 4 in Fig. \ref{fig:GMM_cov_mat}(a)) but also between them: clusters 1 in (a) and 0 in (b), 5 in (a) and 6 in (b), etc. Plotting these clusters in physical space gives the results in Figure \ref{fig:GMM_clusters}. From fluid dynamics theory, Callaham et al. were able to have names for each clustered region in Figure \ref{fig:GMM_clusters}(a), these will be described later in Figure \ref{fig:balance_models}. As can be seen, the results do differ, with the alternative code needing 7 clusters to identify the transitional layer (green in (a) and purple in (b)), instead of 6 for the original code. The difference could be explained by either the different convergence criteria or some of the covariance matrix regularizations which is performed in the \texttt{sklearn} implementation\cite{sklearnGMM}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \caption{Covariance matrices of for each of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm. The colorscale is not too important, as the magnitude of the covariance matrix is more important.}
  \label{fig:GMM_cov_mat}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithms. The cluster numbers match the ones in Figure \ref{fig:GMM_cov_mat}, with the difference of not being 0-indexed (Cluster 1 here is cluster 0 in Fig. \ref{fig:GMM_cov_mat})}
  \label{fig:GMM_clusters}
\end{figure}

\newpage

Then comes applying SPCA to each cluster to identify which terms are active in it. Due to the different clustering obtained, the optimal alpha values needed changing between the methods. For the original code, the optimal alpha value was set to 10, but had to be set to 7 for the alternative code, in order to keep the purple cluster in Fig. \ref{fig:balance_model_clusters}(b) from being combined with the brown one. The results of the SPCA algorithm are $n_{clusters}$ sets of active terms. In the case where some clusters have the same active terms, they are combined. This results in a set of unique balance models, which are plotted in Figure \ref{fig:balance_models}. As the turbulent boundary layer is a well-studied physical phenomenon, clusters in Fig. \ref{fig:balance_models} and \ref{fig:balance_model_clusters} have been given names to describe the dynamics at play in them. From these plots, the alternative code was able to similarly identify important dynamics in the boundary layer. As in Figure \ref{fig:GMM_cov_mat}, some clusters are well identified by both methods. In both methods, the spatial arrangements of the clusters are the same, though their dynamics sometimes differ, or even switch, with the orange cluster in Fig. \ref{fig:balance_models} having the same dynamics but describing two different regions in Fig. \ref{fig:balance_model_clusters}. Overall, similar key dynamics are identified in most of the clusters. For example, the viscous sublayers in blue (a) and green (b) of Fig. \ref{fig:balance_model_clusters} has the viscous forces and streamwise Reynolds’ stress terms as active in both cases. The inflow region in both cases has the streamwise Reynolds’ stress term as inactive, which is the main characteristic of that region. And in both cases, the transitional layer is identical (purple cluster).


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_balance_models.png}
      \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_balance_models.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance models found after applying SPCA, when using the original Callaham et al. code (a) and an alternative code (b). Here, the cluster colors will match the ones in Figure \ref{fig:balance_model_clusters}. Multiple regions are identified. The inflow region (red): where the flow is still laminar, and the streamwise Reynolds stress term is inactive. The free-stream region (green): where only inertial and pressure gradient forces matter, and the viscous forces are inactive. The inertial sublayer (orange): where the inertial forces are the most active. The transitional layer (purple): where the flow is transitioning from laminar to turbulent, hency why a lot of the terms are active. Finally, the viscous sublayer (blue): where the flow is dominated by the viscous forces.}
  \label{fig:balance_models}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_spca_clustering_space.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_spca_clustering_space.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance model clusters in physical space, when using the original Callaham et al. code (a) and an alternative code (b). \textbf{(a)} Identified balance models include a laminar inflow region (red), a free-stream region (green), an inertial sublayer (orange), a transitional layer (purple), and a viscous sublayer (blue). \textbf{(b)} Identified balance models include an inflow region with low Reynolds' stress (brown), a free-stream region (orange), an inertial sublayer with stream-wise and wall normal inertial forces (red), a transitional layer (purple), and a viscous sublayer (green and blue). The cluster colors here match the ones in Figure \ref{fig:balance_models}}
  \label{fig:balance_model_clusters}
\end{figure}

Overall, there is a good agreement between the results, considering differences in the specifics of the Gaussian Mixture Modelling clustering algorithm. The alternative code was able to reproduce the identification of important dynamics in the boundary layer. It is also important to note that, though the overall method presented in Callaham et al. (2021)\cite{callaham2021learning} brings large improvements in terms of generalizing the identification of dominant balance regimes for different cases, there are still two parameters to set: the number of clusters to find when using the GMM algorithm, and the alpha value for the SPCA algorithm. These parameters can have a large impact on the results obtained. This is a key point and will be further discussed in the Stability Assessment section.

\newpage

\section{Exploration of other algorithms}

In the interest of further testing Callaham et al.’s method, and following the Supplementary Discussion in the Supplementary Information of the paper\cite{callaham2021learning}, other clustering algorithms could be used compared to Gaussian Mixture Models. This section explores a few options and discusses the results obtained.

\subsection{Spectral clustering}

Because of the nature of the data being dealt with, the similarity measure used by the clustering algorithm being Euclidean may not be the most appropriate\cite{luxburg2007tutorial}. As a result, spectral clustering is a convincing candidate for this, and it was also suggested in the Supplementary Discussion of the paper\cite[Supplementary Information]{callaham2021learning}.

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/SC_CovMat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/SC_clustering_space_6.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/SC_balance_models_6_1.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/SC_spca_clustering_space_6_1.png}
      \subcaption{}
  \end{minipage}

  \caption{Results of the Spectral Clustering algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Spectral Clustering algorithm. \textbf{(b)} Plot of the clusters found by the Spectral Clustering algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include a viscous sublayer (orange), an inertial sublayer (green), a transitional and inflow layer (red), and a no-dynamics free-flow region (red).}

  \label{fig:SC_results}
\end{figure}

When using spectral clustering, a significant drawback is the computational complexity, which is at worst $\mathcal{O}(n^3)$, where n is the number of samples. This makes the algorithm less efficient for larger datasets, which are common in physical systems. Furthermore, it is not possible to sequentially add new points to the graph. By training it on a small fraction (0.01) of the full dataset, the results are shown in Figure \ref{fig:SC_results}. Because prediction of cluster membership for the rest of the dataset is hard, the solution was to plot the cluster memberships as a scatter plot. Overall, spectral clustering deals well with the different-shaped clusters, as the main structure in the flow was captured. However, looking at the dynamics identified, it is once again the case that some of the key dynamics are identified, notably the inertial (green) and viscous sublayers (orange). However, the inertial sublayer here extends vertically into the region identified by the original code as the laminar inflow region. Moreover, a large cluster, which spans the previously identified transitional and laminar inflow region, is simply considered to have all terms active (except the wall-normal stress term). One other novelty is that the free-flow region was identified to have no active terms.

Therefore, though it provides a non-Euclidean based clustering algorithm, spectral clustering is not well suited for this case study, or at least the aims of the method proposed by Callaham et al. (2021)\cite{callaham2021learning}. It is computationally expensive, cannot accept new data points, and additionally can require one extra parameter to set (\texttt{n\_neighbours}) when using nearest neighbors to build the graph.


\subsection{K-Means}

The next algorithm used is the K-Means algorithm. Compared to spectral clustering, it is much less computationally expensive and is able to accept new data points. The main drawback is that it may not be best suited for any physical system. For example, in the case of the turbulent boundary layer, the clusters are not spherical. More importantly, a great majority of points are near the origin, and K-Means clustering will struggle to distinguish that group of points as multiple clusters. One workaround used here is to set a higher number of clusters to find, which may force the algorithm to identify the different clusters in the cloud of points near the origin.

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/KMeans_CovMat_14.png}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/KMeans_Clustering_Space_14.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/KMeans_balance_models_14_8.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/KMeans_spca_clustering_14_8.png}
      \subcaption{}
  \end{minipage}
  \caption{Results of the K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the K-Means algorithm. \textbf{(b)} Plot of the clusters found by the K-Means algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include an inflow region (purple), a inertial sublayer and inflow region (orange), a transitional layer (green), and a region that spans the free-flow region and the viscous sublayer(blue).}

  \label{fig:KMeans_results}
\end{figure}

The results of the K-Means algorithm are shown in Figure \ref{fig:KMeans_results}. Using the larger cluster number did help in identifying distinct regions; however, the dominant balance regimes identified in them do differ significantly from the ones identified by the original code. Again, clusters with similar dominant balances to those originally identified extend into other regions that had been identified to have very different dominant balance regimes (orange cluster in Figure \ref{fig:KMeans_results}).

\subsection{Weighted K-Means}

One solution to K-Means relying solely on Euclidean distance is to apply a weight to the samples. This is done to give more importance to the samples that are closer to the origin, essentially mimicking the effect of spreading them apart. This should encourage the algorithm to identify multiple clusters in the cloud of points near the origin. The weights are set to depend on the distance to the origin as follows:

\begin{equation}
  w_i = 1 - (\text{tanh}^{2}(\frac{1}{2}|\vec{OX}|)) \text{, where } \vec{OX} \text{ is the vector from the origin to the sample} x_i
\end{equation}

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_CovMat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_clustering_space_6.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_balance_models_6_7.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_spca_clustering_space_6_7.png}
      \subcaption{}
  \end{minipage}
  \caption{Results of the Weighted K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Weighted K-Means algorithm. \textbf{(b)} Plot of the clusters found by the Weighted K-Means algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include an inflow region (purple), an inertial sublayer and inflow region (orange), a transitional layer (green), a region that spans the free-flow region and the viscous sublayer(blue), and finally, a region that spans the viscous sublayer (red)}
  \label{fig:WKMeans_results}
\end{figure}

The results of the Weighted K-Means algorithm are shown in Figure \ref{fig:WKMeans_results}. The results are quite similar to standard K-Means; however, they were obtained by having the GMM find 6 clusters only. Once again, some of the key dominant balance regimes are identified, but they are not assigned to the same locations as in the original code.

\subsection{Summary}

Overall, the Gaussian Mixture Model clustering algorithm offers really good flexibility thanks to its small number of hyperparameters, and ability to predict cluster membership for new data points. Additionally, it is able to identify clusters of different shapes and sizes, which is important when differentiating different dominant balances close to the origin. Another possible path to explore could be a mixture of other distributions, e.g., Cauchy.



\section{Stability Assessment}

Though Callaham et al. (2021)\cite{callaham2021learning} presents a well-generalizable method for unsupervised identification of dominant balance regimes, there are still two hyperparameters to set: the number of clusters to find when using the GMM algorithm, and the alpha value for the SPCA algorithm. These can have a large impact on the results obtained. This section aims to explore the stability of the results obtained when changing these hyperparameters and discuss the implications of this when using the method for previously unstudied physical systems.

\subsection{Under different number of clusters set}

The first test that can be carried out is to see how results differ when setting a different number of clusters. Ideally, the cluster number set should not matter greatly. With the exception of very small numbers and approaching the limit: $n_{clusters} \approx n_{samples}$,  the results should be similar thanks to applying Sparse PCA and combining clusters with identical dominant balances. However, this is not guaranteed. If one were to combine two clusters with 2 terms active, and one shared active term, the resulting cluster would have the 3 terms active. Inversely, by increasing the number of initial clusters, this could result in dividing clusters into smaller, unidentically balanced clusters.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_cluster_numbers_bal_mods.png}
  \caption{Plot of the obtained balance models for different initial cluster number. Reading top to bottom and left to right, the cluster numbers are 4 to 15}
  \label{fig:diff_clust_num_bal_mods}
\end{figure}

The algorithm was run for multiple cluster numbers, and the balance models were obtained for each case. These are all plotted in Figure \ref{fig:diff_clust_num_bal_mods}. It can be seen that the results are mostly stable, with a slow increase in the number of final unique balance models found (see Fig. \ref{fig:diff_clust_num_nmodels}), but with identical balances identified, except for low cluster numbers. Importantly, the relevant case of 6 clusters seems to be unique and changes quite a lot for the 5 and 7 clusters case.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{../Plots/Stab_Ass/different_cluster_numbers_nmodels.png}
  \caption{Plot of the number of unique balance models identified for different number of cluster numbers}
  \label{fig:diff_clust_num_nmodels}
\end{figure}

However, checking the actual clustering in space in Fig. \ref{fig:diif_clust_num_clustering} and comparing to the balance models obtained, it can be seen that having identified the same balance models does not necessarily translate to the exact same clustering in space.


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_9.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_14.png}
      \caption{}
  \end{subfigure}
  \caption{Final unique balance models identified when the cluster number was set to 9 (a) and 14 (b).}
  \label{fig:diif_clust_num_clustering}
\end{figure}

\subsection{Under different alpha values}

To test the stability of the algorithm under changes to the second parameter: $\alpha$, for the $\ell$-1 regularization of the SPCA problem, the same test is done. Taking the case where the cluster number was set to 6, the unique dominant balance models were obtained for values of $\alpha$ going from 0.1 to 17. This is a choice based on the SPCA residuals curve obtained for the 6 clusters scenario (see Fig. \ref{fig:spca_residuals}) which shows these values being in an area of acceptable trade-off between sparsity and accuracy.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{../Plots/BL/GMM_spca_residuals.png}
  \caption{Plot of the SPCA residuals for different values of $\alpha$. Recall, the residuals are computed as the $\ell$-2 norm of the inactive terms for all clusters}
  \label{fig:spca_residuals}
\end{figure}

\newpage

Looking at the results in Fig. \ref{fig:diff_alpha_bal_mods}, there is a lot more variation in the results obtained, with as expected a lot more active terms and less balance models for small $\alpha$ values and the inverse case for larger values.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_alpha_bal_mods.png}
  \caption{Plot of the obtained balance models for different $\ell$-1 regularization term factor, $\alpha$. Reading top to bottom and left to right, the cluster numbers are 0.1, 0.2, 0.5, 1, 2, 5, 7, 8, 10, 12, 15, and 17}
  \label{fig:diff_alpha_bal_mods}
\end{figure}

However, for values close to 10, which was selected in the paper, there is some stability in the results, with values from 10 to 15 yielding the same resutls. And checking that it is still the case in physical space (see Fig. )

\begin{figure}[htbp]
\end{figure}

\subsection{Under different training set size}

\subsection{discussion}

\chapter{Elasto-inertial turbulence}

\section{Background}

\section{Methodology}

\section{Results}

\section{Discussion}

\chapter{Conclusion}


\chapter{Appendix}

\section{Algorithms}


\begin{definitionbox}{Algorithm 1: Gaussian Mixture Model clustering}
  \begin{algorithmic}[1]
    \State Data $\vec{x} \in \mathbb{R}^{n \times n_{features}}$, number of clusters/Gaussians to fit $K$ \Comment{Inputs}
    \State Cluster assignments $z \in \mathbb{R}^n$ \Comment{Output}
    \State $w_{k} \gets \frac{1}{k}$ \Comment{Initialisation of the weights}
    \State $\mathbf{\mu_{k}} \gets \mathbf{c_{k}}$ \Comment{Initialise the means as K-Means centroids}
    \State $\mathbf{\Sigma_{k}} \gets \mathbf{\Sigma}_{C_{k}}$ \Comment{Initialise as the covariance matrices of the clusters}
    \State $C_{k} \gets w_{k} \times \mathcal{N}(\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ \Comment{Initialise the clusters/Gaussians}

    \State \textbf{Expectation-Maximisation:}
    \State $\mathcal{L} \gets 0$ \Comment{Initialise the log-likelihood}
    \For{$i < \text{max-iter}$}
      \State \textbf{Expectation step:}
      \For{$k \in K$}
        \State $\mathbf{b_{k}} \gets \frac{w_{k} \times f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})}{\sum_{j=1}^{K} w_{j} \times \mathcal{N}(\mathbf{x}|\mathbf{\mu_{j}}, \mathbf{\Sigma_{j}})}$ \Comment{Compute the responsibilities for each cluster}
      \EndFor
      \State \textbf{Maximisation step:}
      \For{$k \in K$}
        \State $w_{k} \gets \frac{1}{n} \sum_{i=1}^{n} \mathbf{b}_{k}$ \Comment{Update the weights}
        \State $\mathbf{\mu_{k}} \gets (\sum \mathbf{b}_{k}\mathbf{x})/\sum \mathbf{b}_{k}$ \Comment{Update the means}
        \State $\mathbf{\Sigma_{k}} \gets (\sum \mathbf{b}_{k}(\mathbf{x} - \mathbf{\mu}_{k})^{2})/\sum \mathbf{b}_{k}$ \Comment{Update the covariance matrices}
      \EndFor
      \State $\mathcal{L}_{\text{new}} \gets \sum_{i=1}^{n} \log(\sum_{k=1}^{K} w_{k} \times f(x_{i}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}}))$ \Comment{Compute the log-likelihood}
      \State $\epsilon \gets \mathcal{L} - \mathcal{L}_{\text{new}}$ \Comment{Compute the difference in log-likelihood}
      \If{$\epsilon < 10^{-4}$}
        \State \textbf{break}
      \EndIf
      \State $\mathcal{L} \gets \mathcal{L}_{\text{new}}$ \Comment{Update the log-likelihood}
    \EndFor

  \end{algorithmic}
  Where:

  - $f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ is the probability density function of a multivariate normal distribution

  \label{alg:GMM}
\end{definitionbox}

\newpage

\begin{definitionbox}{Algorithm 2: Sparse PCA Algorithm}
  \begin{algorithmic}[1]
    \State Data matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ (n: samples, p: features), number of components $k$, sparsity controlling parameter $\alpha$, maximum iterations $\text{max\_iter}$, tolerance $\text{tol}$ \Comment{Inputs}
    \State Sparse components $\mathbf{V} \in \mathbb{R}^{p \times k}$ \Comment{Outputs}
    \State Initialize $\mathbf{V} \gets \text{random}$, $\mathbf{U} \gets \text{random}$ \Comment{Initialization}

    \State \textbf{Initialization:}
    \State Center the data matrix $\mathbf{X}$ by subtracting the mean of each column
    \State Initialize $\mathbf{V}$ with random values

    \For{$i = 1$ to $\text{max\_iter}$} \Comment{Iterate to optimize components and loadings}
      \State \textbf{Update Loadings:}
      \For{$j = 1$ to $k$} \Comment{Update each loading vector}
        \State $\mathbf{u}_j \gets \arg \min_{\mathbf{u} \in \mathbb{R}^n} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{u}_j\|_1$
      \EndFor

      \State \textbf{Update Components:}
      \For{$j = 1$ to $k$} \Comment{Update each component vector}
        \State $\mathbf{v}_j \gets \arg \min_{\mathbf{v} \in \mathbb{R}^p} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{v}_j\|_1$
      \EndFor

      \State \textbf{Convergence Check:}
      \State Compute the reconstruction error: $\text{error} \gets \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F$
      \If{$\text{error} < \text{tol}$}
        \State \textbf{break}
      \EndIf
    \EndFor

    \State \textbf{Return:} Sparse components $\mathbf{V}$

  \end{algorithmic}
  In practice, \texttt{sklearn} uses a dictionary learning for sparse coding\cite{mairal2010online}, and structured sparse PCA\cite{jenatton2010structured} for the sparse PCA algorithm. But both are attempting to solve the optimization problem: $(U^*, V^*) = \arg_{U, V} \text{min } \frac{1}{2} ||X-UV||_{\text{Fro}}^2+\alpha||V||_{1,1} \text{subject to } ||U_k||_2 <= 1 \text{ for all } 0 \leq k < n_{components}$
  \label{alg:SPCA}
\end{definitionbox}


\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
