\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{setspace} % Add the setspace package
\usepackage{titlesec} % Add the titlesec package for customizing titles

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage[font=small]{caption}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
  breakable,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}[1]{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=#1,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
  breakable,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}[1]{
  colback=green!5!white,    % Light green background
  colframe=green!75!black,   % Darker green frame
  colbacktitle=green!85!black,  % Even darker green for the title background
  title=#1,         % Title text for example box
  fonttitle=\bfseries,    % Bold title font
  enhanced,
  breakable,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\onehalfspacing

\geometry{top=1.5in} % Adjust the value as needed

% Customization for chapter titles
\titleformat{\chapter}[display] % Use 'display' to put number and title on separate lines
  {\normalfont\LARGE\bfseries} % Format for the chapter title
  {Chapter \thechapter} % Display "Chapter X"
  {0.5em} % Space between "Chapter X" and the title
  {\Huge} % Chapter title format
\titlespacing*{\chapter}{0pt}{-20pt}{20pt} % Adjust spacing around chapter title

% ----------------------------------------------------------------



\begin{document}

\begin{titlepage}
  \centering
  {\LARGE\bfseries MPhil DIS Project 24\par}
  {\LARGE Learning Dominant Physical Processes with Data-Driven Balance Models\par}
  \vspace{1cm}
  {\includegraphics[width=0.2\textwidth]{University_Crest.pdf}\par}
  {\Large CRSiD:\ tmb76\par}
  \vspace{1cm}
  {\Large Department of Physics\par}
  {\Large\bfseries University of Cambridge\par}
  \vfill
  {\itshape Submitted in partial fulfilment of the requirements of the MPhil degree in Data Intensive Science}
  \vfill
  {\large Hughes Hall  \hspace{6cm} \today\par}
\end{titlepage}

\chapter*{Acknowledgements}

The author would like to first and foremost thank Dr. Richard Kerswell for his guidance and support throughout this project. His advice was short and to the point, and it always managed to give a clear new perspective on the project, which was extremely helpful. The author would also like to thank Miguel Beneitez for his help in getting to grips with the Elasto-Inertial Turbulence data, and for taking the time to get it ready.

\tableofcontents

\chapter{Executive Summary}


\chapter{Introduction}


For many problems in engineering and physical sciences, equations involve a large number of terms and complex differential equations. Simulating them can be computationally expensive or unnecessarily so, as there exists regions where only a subset of the terms dominate the equation. In such cases, one can simplify the equations to a balance between these dominant terms, and simulate the system with sufficient accuracy and relatively lower computational cost \cite{charney1990scale}. This method, known as dominant balance or scale analysis, has been a powerful tool in physics.

\vspace{5mm}

Though extremely useful, dominant balance usually requires expertise and is mostly done by hand through time-consuming proofs. This report discusses and verifies a novel approach, developped by Callaham et al. (2021)\cite{callaham2021learning}, which explores using data from a physical system and machine learning methods to identify dominant balances algorithmically.

\vspace{5mm}

Like for any research, one of the key steps of the scientific method is reproducibility. Results must be reproducible by others, ensuring that the same conclusions can be drawn multiple times. Otherwise, it may be considered erroneous, or simply a random occurence. This project therefore has for a core aim to evaluate the reproducibility of the results of the Callaham et al. (2021)\cite{callaham2021learning} paper and to test the robustness of their method. First discussing the research surrounding the paper, the report will then delving into the details of the method. Reproducing the results for one of the case studies covered in the paper with the use of alternative code will then be explored, additionally exploring the use of other algorithms than the method's chosen one. Finally, the method will be used on a new dataset, hopefully shedding some light on a flow called Elasto-Inertial Turbulence.



\chapter{Background}


As aforementionned, dominant balance or scale analysis is a powerful tool in simplifying the modelling of physical processes. Importantly, it helps better understand the physics at play in a system. By identifying the subset of terms that truly matter in an equatione, one can deal with easier computations by avoiding unnecessary complications of the model. Taking the example of meteorology, modelling the entire atmosphere using the full Navier-Stokes equations of motion for all scales would have an immense computational cost. And a large amount of improvements in Numerical Weather Predicitons can be attributed to scale analysis \cite{charney1947dynamics, phillips1963geostrophic, burger1958scale, yano2009scale}.

\vspace{5mm}

However, this process can be slow as it requires considerable expertise from researchers. And for most of the well studied physical systems, this was done by hand over decades (cf. above references). But with the wealth of computational power and data science techniques nowadays, an attempt at automating dominant balance can be made. First is the Portwood et al. (2016)\cite{portwood2016robust} paper which used a cumulative distribution function on local density gradients to separate each region of a stratified turbulent flow. The method used was highly tailored for its case study, with the gradient of one of the terms being used, knowing it had dynamics discerning qualities. And results were interpreted through the knowlegde of the authors. Second is the work carried out in Lee \& Zaki (2018)\cite{lee2018detection} where an algorithm to detect different dynamical regions is introduced. Again, this is through the use of case-specific variables (vorticity), which restrict the use of this algorithm to certain flows. Finally, Sonnewald et al. (2019)\cite{sonnewald2019unsupervised} used a K-Means clustering algorithm to identify dynamically distinct regions in the ocean. Here, they introduced the idea of using the terms in the governing equations as features. However, the identification of active terms is done through comparison of the magnitudes of each of the terms in the equation. In other words, identifciation of dominant terms is not done algorithmically but ``manually''. Thus, these methods are mostly designed for specific case studies and partly rely on expert knowledge to interpret the results.

\vspace{5mm}

A similar challenge in data science and machine learning has been to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009)\cite{schmidt2009distilling} contributed to a breakthrough using symbolic regression to find linear and non-linear differential equations. And this was improved in Brunton et al. (2016)\cite{brunton2016discovering}, approaching with a less expensive sparse regression, which for high-dimensional problems means identifying a sparse governing equation. This depended on governing equations usually having only a subset of terms being important, as in dominant balance. Lejarza \& Baldea (2022)\cite{lejarza2022data} further advanced this by using multiple basis functions and a non-linear moving horizon optimization algorithm to learn governing equations from noisy data. Deep learning methods have also been used in this effort. First, where the lagrangians are learned, therefore learning how to model complex physical systems, and learning symmetries and conservation laws, where other networks failed \cite{cranmer2020lagrangian}. Second, deep learning (Graph Neural Network) and symbolic regression are combined to create a general framework to recover equations of physical systems \cite{cranmer2020discovering}. This method has the advantage of being generalisable and therefore useable to extract plausible governing equations for unknown systems.

\vspace{5mm}

This generalisable quality is precisely the gap that Callaham et al. (2021)\cite{callaham2021learning} attempt to fill in the identification of dominant balance models. They propose a novel approach to take in simulated or measured data from virtually any physical system, and extract dominant balance models with minimal user input. This means one could use it in conjunction with the above governing equation identifying methods, and explore plausible governing equations and asymptotic regimes of an unknown physical phenomena. Though this is a very powerful result, as Schmidt \& Lipson (2009)\cite{schmidt2009distilling} noted for their work, this method should be seen as a guiding tool to help indicate where scientists should focus their attention, rather than a definitive answer.


\chapter{Methodology}

The method proposed by Callaham et al. (2021)\cite{callaham2021learning} can be summarized in three steps. And this section aims to delve into the details of each of these steps.

\section{Data \& Equation Space Reprensentation}

For this method, obtaining the data can be done through simulations or measurements, with almost no restrictions as to what equations can be studied. The data typically comes in the form of space-time fields of physical variables: $u(\vec{x}, t)$, where $u$ is the physical variable of interest (e.g. $\vec{u}$, $p$, etc. for the Navier-Stokes equations). It is essential that one can compute all the terms of the governing equation from the variables. Additionally, the computed terms must all balance out to 0 for each point in time and space. This relies on having the computated terms be as they appear in the equation, which ensures a linear covariance structure, as each term will be balanced by a linear combination of the other terms\cite[Supplementary Information]{callaham2021learning}.

\vspace{5mm}

The fundamental idea of the Callaham et al. (2021) method is to take these fields of terms in the physical-space and organize them into an equation-space, where each dimension or feature is one of the terms in the equation, and each sample is a point in space and time. Thus, each sample will be a vector $\vec{f} \in \mathbb{R}^k$ where $k$ is the number of terms in the equation, such that:

\begin{equation}
    \vec{f} = \begin{bmatrix} f_1(u(\vec{x}, t), \hdots) \\ f_2(u(\vec{x}, t), \hdots) \\ \vdots \\ f_k(u(\vec{x}, t), \hdots) \end{bmatrix}
\end{equation}

Where $f_i$ is the $i^{th}$ term in the equation, itself a function of the physical variables. Again, one must ensure that for each sample, the terms all balance out to a residual at least several orders of magnitude smaller than the terms themselves. This is in order to make sure that the equation studied or data used is not invalid.


\section{Gaussian Mixture Model Clustering}

By clustering the points in equation space, groups of points that have a similar balance of terms can be identified. Here, the chosen algorithm is the Gaussian Mixture Model (GMM) clustering algorithm. This algorithm relies on the assumption that the data has been generated from a mixture of a finite number of Gaussian distributions with unknown parameters\cite{mit2015algorithmic}. It has the advantage of being able to identify clusters with varying shapes and sizes. It only requires one hyperparameter, which is the number of clusters the algorithm must find, and therefore how many Gaussian distributions the data is assumed to have been generated from\cite{sklearnGMM}. This number is not fixed by the method so must be chosen for each case study. Generally, the choice should be conservative, aiming for a number of clusters that is likely to be greater than the true/expected number of dominant balance regimes in the system. This is because if the number is higher than it needs to be, it will likely be the case that the next steps in the method will identify some of the clusters as having identical dominant balances, and these will be combined.

\vspace{5mm}

The way GMMs fit to the data is by using the Expectation-Maximisation algorithm. This algorithm starts from an initial guess for the parameters of the gaussian distributions: $w_0$, the weight for that distribution, $\mathbf{\mu}_{0}$, the vector mean of size $[1 \times n_{features}]$, and $\mathbf{\Sigma}_{0}$, the $[n_{features} \times n_{features}]$ covariance matrix, for each Gaussian. The expectation step evaluates the likelihood of a given point belonging to each cluster, for the current parameters. And the maximisation step updates the parameters with weighted averages of those posterior probabilities\cite{dempster1977maximum} (see Alg. 1, in section \ref{alg:GMM}). This converges to the maximum likelihood estimates of the parameters (see Fig. \ref{fig:GMM_example}). Once fitted, cluster membership of new data points can be determined by taking the cluster with the highest probability at that point.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{./GMMexample 1.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 2.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 3.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 4.png}
  \caption{Example of a Gaussian Mixture Model fit to data. The data is generated from 3 Gaussian distributions, and the GMM algorithm fits 3 Gaussians to it. \cite[1D Example Notebook]{gmm_towardsdatascience}}
  \label{fig:GMM_example}
\end{figure}

\vspace{5mm}

Another key output of the GMM is in its probabilistic nature as each Gaussian is defined by a covariance matrix of the equation's terms. This gives insight into which terms could be active in each cluster, and helps illustrate the intuition behind identifying active terms geometrically with this method. If only a subset of the terms in a cluster have non-zero covariance, then it means that it is only along that subset of dimensions that the cluster varies, and hence that the terms are likely to be non-negligible.

\section{Sparse Principal Component Analysis}

With the points grouped in clusters of similar balance of terms, the next step is to identify which terms are active and which can be dropped to simplify the equation for that cluster. This could be done by applying a theshold to the covariance matrices obtained from the GMM clustering. But it is done more robustly through Sparse Principal Component Analysis (SPCA). SPCA is a variant of Principal Component Analysis (PCA) which is a method used to reduce the dimensionality of the data, whilst maximising the information retained. The new dimensions onto which the data is projected are called principal components and are the directions in which the data has the greatest variance\cite{lever2017principal} (see Fig. \ref{fig:PCA_wiki}). SPCA differs from PCA in that it adds a sparsity constraint to the principal components, adding a $\ell$-1 penalty to the objective function of PCA for the number of non-zero coefficients. This then leads to some coefficients being shrunk down to 0 when the $\ell$-1 penalty factor ($\alpha$) is large enough\cite{zou2006sparse} (see Alg. 2, in section \ref{alg:SPCA}). When setting a low value for $\alpha$, SPCA will be more lenient in judging if a term is active in a cluster. The higher the value, the sparser the principal components will be. Too low, and all clusters will have a full balance, with all terms active. Too high, and all clusters will have an empty balance, with no terms active.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{./pca_wiki.png}
  \caption{Example of a 2D dataset projected onto its first 2 principal components (black arrows). The first principal component is the direction of greatest variance, and the second is orthogonal to it. \cite{pca_wikipedia}}
  \label{fig:PCA_wiki}
\end{figure}

Taking the GMM clustered data, the SPCA algorithm is applied to each cluster, and only the leading component is extracted. By taking the leading component, the algorithm is identifying the one direction along which there is the most variance within that cluster. This will indicate which terms vary the most within that cluster, and therefore which ones matter more. Then applying the sparsity constraint, the vector obtained will have some of its coefficients shrunk to 0, and the non-zero coefficients will indicate the active terms in that cluster.

\vspace{5mm}

To help judge what value of $\alpha$ to pick, Callaham et al. (2021) propose to apply SPCA for a range of $\alpha$ values. Then a residual error is computed, and is defined as the $\ell$-2 norm of the neglected terms across all clusters. This illustrates the trade-off between sparsity (simplifying the model) and retaining information (keeping the model accurate), helping to pick an optimal value (see Fig. \ref{fig:spca_residuals}). Once SPCA is applied, each cluster is then associated to a sparse binary vector defining their active terms (1 if active, 0 if not). The balance models are then checked for uniqueness, and if some clusters have the same balance, they are combined.


\chapter{Conducted research}

To test and validate this method, Callaham et al.\ used a series of case studies\cite{callaham2021learning}. In this chapter, the aim is to discuss the reproducibility of the results presented in the paper, as well as to test and discuss its drawbacks.

\section{Portability of the code}

One great quality of the Callaham et al. (2021)\cite{callaham2021learning} paper is the sharing of their code in the form of runnable notebooks. This is tied to the motivation for this project being the importance of reproducibility in today’s code-rich research environment. It is key that scientists share their code so one may verify how the results were obtained. This is also a great asset as it allows for a more thorough understanding and test of the method, by writing explicitely different code.

\vspace{5mm}

Overall, the code is of great quality, though some portability issues were encountered. Some of the dependencies were not stated in their README. More importantly, it seemed some of the data-generating code was modified between running their notebooks for the final time and uploading it to the repository. This led to some troubleshooting to get the data to match the one used in the paper. Unfortunately, for the flow past a cylinder case, a file needed in the setup of the Nek5000 simulation software was missing\cite{nek5000setup}. For the bursting neuron case, the times for which the data was generated was wrongly set, not starting from $t = 0$ s. Similarly, for the Gulf of Mexico's currents data, the remote reading code was selecting snapshots of the data that did not match the ones used in the paper, and the first 45 were set to zero, which had no real use. Further, the results in the paper were actually obtained from a different snapshot than the one stated in the paper. For this report, the data was simply downloaded from the HYCOM database \cite{hycom}, and the time was set to the same as stated in the paper.

\vspace{5mm}

Nevertheless, the code was otherwise easy to run, and the notebook format is very useful to better understand the logic of the code. Following this code, the code was first written for the turbulent boundary layer case. And explicitly alternative code was then written to test the reproducibility of the results.

\section{The turbulent boundary layer case}

To ensure that none of the main results from Callaham et al. are due to a “lucky” bug in their code, and were obtained through random chance, alternative code is written which, in practice, performs the same functions. Also, because the underlying method is essentially the same for all case studies, it was decided to put particular focus on one of the cases: the turbulent boundary layer.

\subsection{Reproducing the code}

Throughout the Callaham et al. (2021)\cite{callaham2021learning} notebooks, aside from the three main steps of the method, the large majority of the code involves handling the data, switching from equation space to physical space, and obtaining the unique balance models. Most of it is written using \texttt{Numpy}, which is arguably a very good choice for efficient and trustworthy numerical operations. In the alternative code, however, \texttt{Pandas} was primarily used. In terms of performance, it is close to \texttt{Numpy}, especially for the size of the data in this study, and provides similar practical functions. For some code sections, however, impractical workarounds were required, particularly to replace the \texttt{numpy.unique} method, when finding and combining duplicate balance models.

\vspace{5mm}

The data for this case study is obtained from the John Hopkins database, which conducted a direct numerical simulation of a boundary layer over a stationary plate\cite{jhtdb}. This database provides the $x$ and $y$ coordinates as well as the following variables: $\overline{u}$, the streamwise component of velocity;  $\overline{v}$, the wall-normal component of velocity;  $\overline{p}$, the fluid pressure; and $(\overline{u{\prime} v{\prime}})$ and $(\overline{u{\prime}^2})$, the wall-normal and streamwise Reynolds stress terms, respectively. From these variables, the terms of the streamwise component of the Reynolds-Averaged Navier-Stokes (RANS) equations need to be calculated (see Eq. (5.1)).

\begin{equation}
  \bar{u} \bar{u}_x + \bar{v} \bar{u}_y = \rho^{-1} \bar{p}_x + \nu \nabla^2 \bar{u}  - (\overline{u' v'})_y - (\overline{u'^2})_x
\end{equation}

To compute the derivative terms, the method employed by Callaham et al. is based on \texttt{scipy.sparse}’s sparse matrices library. The aim is to build a derivative operator sparse matrix so that when computing the matrix product of that matrix with the fields or variables, the second-order forward/central/backward difference derivative is obtained, following the method described in ‘Fundamentals of Numerical Computations’\cite{finitediff}. Though it has the advantage of being portable for all variables, it is much slower than using the \texttt{numpy.gradient} function, which performs the same computations, albeit more explicitly, when setting the \texttt{edge\_order} argument to 2. With the derivatives computed, spatial fields of each term in the equation can be plotted (see Fig. \ref{fig:RANS_terms}).

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using the original Callaham et al. code}
  \label{fig:RANS_terms}
\end{figure}

\vspace{5mm}

The next step is to use a Gaussian Mixture Model (GMM) to cluster the data in equation space. Callaham et al. judiciously chose to use the \texttt{sklearn} library’s implementation of the GMM algorithm. This implementation is likely very efficient, being written and optimized by experienced developers. With the chosen arguments, the algorithm is initialized using the \texttt{‘k-means++’} method. This method initializes the means of the Gaussians using the centroids found by the K-Means clustering algorithm (see Algorithm 3), which is itself initialized based on the empirical probability distributions of the data\cite{arthur2007kmeans}. The covariance matrices are then initialized as the covariance matrices of the clusters found by the K-Means algorithm. The algorithm then uses the Expectation-Maximisation algorithm as expected. Convergence is evaluated using the log-probability of the data:

\begin{equation}
  \log \mathcal{L}(\vec{X} | \vec{\theta}) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} w_k \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \vec{\Sigma}_k) \right)
\end{equation}

The algorithm stops when the difference between the new and old log-likelihood is less than $10^{-3}$.

The alternative code was thus written aiming to follow the same initialization, explicitly using \texttt{sklearn}’s K-Means algorithm with \texttt{‘k-means++’} initialization. The Expectation-Maximization algorithm is then implemented as described in Algorithm 1, using the total log-likelihood difference as the convergence criterion.

For Sparse Principal Component Analysis (SPCA), the \texttt{sklearn} library’s implementation is used. The method employed is based on an extension of sparse PCA, which utilizes structured regularization to constrain the sparsity patterns. This is also done for the alternative code, as the \texttt{sklearn} implementation employs high level methods which were hard to reproduce\cite{jenatton2010structured,mairal2010online}. However, it was decided to use a parallelising package to speed up the computation of the SPCA residuals. Here, the \texttt{joblib} package was used, providing a simple way to distribute jobs\cite{joblib}.

With SPCA completed, the active terms can be identified. From this point, the original and alternative code only differ in which library they predominantly use. The original code primarily uses \texttt{Numpy}, while the alternative code uses \texttt{Pandas}. Alternative code was thus also written in handling the balance model results, getting the unique ones, and plotting them in space.


\subsection{Results}

First, checking the computation of the RANS equation's terms, the \texttt{scipy.sparse} method's obtained terms are plotted in Fig. \ref{fig:RANS_terms}. The alternative code, which used \texttt{numpy.gradient} instead, gave the same results (see Fig. \ref{fig:custom_RANS_terms}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/custom_RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using an alternative code}
  \label{fig:custom_RANS_terms}
\end{figure}

The next step involves clustering the data in equation space. A clear way of comparing results is to examine the obtained covariance matrices. In Figure \ref{fig:GMM_cov_mat}, the covariance matrices obtained when using \texttt{sklearn}’s Gaussian Mixture Model (GMM) routine, as well as when using the custom GMM implementation are shown. It can already be seen that some clusters clearly have the more important terms. This is the case within each method (Cluster 1 and 3 in Fig. \ref{fig:GMM_cov_mat}(a)) but also between them: clusters 1 in (a) and (b), or 5 in (a) and 6 in (b), etc. Plotting these clusters in physical space gives the results in Figure \ref{fig:GMM_clusters}. From fluid dynamics theory, Callaham et al. were able to assign names for each clustered region in Figure \ref{fig:GMM_clusters}(a), and these will be discussed later with Figure \ref{fig:balance_models}. As can be seen, the results do differ, with the alternative code needing 7 clusters to identify the transitional layer (purple in Fig. \ref{fig:GMM_clusters}(a) \& (b)), instead of 6 for the original code. The difference could be explained by either the different convergence criteria or some of the covariance matrix regularizations which is performed in the \texttt{sklearn} implementation\cite{sklearnGMM}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_cov_mat_6.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_cov_mat_7.png}
      \caption{}
  \end{subfigure}
  \caption{Covariance matrices of for each of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm. The colorscale is not too important, as the magnitude of the covariance matrix is more important.}
  \label{fig:GMM_cov_mat}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_clustering_space_6.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_clustering_space_7.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithms. The cluster numbers match the ones in Figure \ref{fig:GMM_cov_mat}, with the difference of not being 0-indexed (Cluster 1 here is cluster 0 in Fig. \ref{fig:GMM_cov_mat})}
  \label{fig:GMM_clusters}
\end{figure}

\newpage

Then comes applying SPCA to each cluster to identify which terms are active in it. Due to the different clustering obtained, the optimal alpha values needed changing between the methods. For the original code, the optimal alpha value was set to 10, but had to be set to 7 for the alternative code, in order to keep the purple cluster in Fig. \ref{fig:balance_model_clusters}(b) from being combined with the brown one. The results of the SPCA algorithm are $n_{clusters}$ sets of active terms. In the case where some clusters have the same active terms, they are combined. This results in a set of unique balance models, which are shown color-coded in Figure \ref{fig:balance_models}. As the turbulent boundary layer is a well-studied physical phenomenon, clusters in Fig. \ref{fig:balance_models} and \ref{fig:balance_model_clusters} have been assigned names to describe the dynamics at play in them. From these plots, the alternative code was able to similarly identify important dynamics in the boundary layer. As in Figure \ref{fig:GMM_cov_mat}, some clusters are well identified by both methods. The two spatial arrangements of the clusters are the same, though their dynamics sometimes differ, or even switch, such as the orange cluster in Fig. \ref{fig:balance_models} having the same dynamics but describing two different regions in Fig. \ref{fig:balance_model_clusters}. Overall, similar key dynamics are identified in most of the clusters. For example, the viscous sublayers in blue (a) and green (b) of Fig. \ref{fig:balance_model_clusters} has the viscous forces and streamwise Reynolds’ stress terms as active in both cases. The inflow region in both cases has the streamwise Reynolds’ stress term as inactive, which is the main characteristic of that region. And in both cases, the transitional layer is identical (purple cluster).


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_balance_models_6_10.png}
      \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_balance_models_7_7.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance models found after applying SPCA, when using the original Callaham et al. code (a) and an alternative code (b). Here, the cluster colors will match the ones in Figure \ref{fig:balance_model_clusters}. Multiple regions are identified. The inflow region (red): where the flow is still laminar, and the streamwise Reynolds stress term is inactive. The free-stream region (green): where only inertial and pressure gradient forces matter, and the viscous forces are inactive. The inertial sublayer (orange): where the inertial forces are the most active. The transitional layer (purple): where the flow is transitioning from laminar to turbulent, hency why a lot of the terms are active. Finally, the viscous sublayer (blue): where the flow is dominated by the viscous forces.}
  \label{fig:balance_models}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_spca_clustering_space_6_10.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_spca_clustering_space_7_7.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance model clusters in physical space, when using the original Callaham et al. code (a) and an alternative code (b). \textbf{(a)} Identified balance models include a laminar inflow region (red), a free-stream region (green), an inertial sublayer (orange), a transitional layer (purple), and a viscous sublayer (blue). \textbf{(b)} Identified balance models include an inflow region with low Reynolds' stress (brown), a free-stream region (orange), an inertial sublayer with stream-wise and wall normal inertial forces (red), a transitional layer (purple), and a viscous sublayer (green and blue). The cluster colors here match the ones in Figure \ref{fig:balance_models}}
  \label{fig:balance_model_clusters}
\end{figure}

Overall, there is a good agreement between the results, considering differences in the specifics of the Gaussian Mixture Modelling clustering algorithm. The alternative code was able to reproduce the identification of pivotal dynamics in the boundary layer. It is also important to note that setting the value of the hyperparameters (number of clusters, alpha value) played a key role here. These parameters can have a large impact on the results obtained. This is a key point and will be further discussed in the Stability Assessment section.

\newpage

\section{Exploration of other algorithms}

In the interest of further testing Callaham et al.’s method, and following the Discussion in the Supplementary Information of the paper\cite{callaham2021learning}, other clustering algorithms could be used compared to Gaussian Mixture Models. This section explores a few options and discusses the results obtained.

\subsection{Spectral clustering}

Because of the nature of the data being dealt with, the similarity measure used by the clustering algorithm being Euclidean may not be the most appropriate\cite{luxburg2007tutorial}. As a result, spectral clustering is a convincing candidate for this, and it was also suggested in the Supplementary Discussion of the paper\cite[Supplementary Information]{callaham2021learning} (see Alg. 3, in section \ref{alg:Spect_Clust}).


\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/SC_CovMat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/SC_clustering_space_6.png}
    \subcaption{}
  \end{minipage}

  \caption{Results of the Spectral Clustering algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Spectral Clustering algorithm. \textbf{(b)} Plot of the clusters found by the Spectral Clustering algorithm.}
  \label{fig:SC_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/SC_balance_models_6_1.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/SC_spca_clustering_space_6_1.png}
      \subcaption{}
  \end{minipage}

  \caption{More results for the Spectral Clustering algorithm method. \textbf{(a)} Plot of the unique balance models found after applying SPCA. \textbf{(b)} Plot of the unique balance model clusters in physical space. Identified balance models include a viscous sublayer (orange), an inertial sublayer (green), a transitional and inflow layer (red), and a no-dynamics free-flow region (red).}
  \label{fig:SC_results_2}
\end{figure}


When using spectral clustering, a significant drawback is the computational complexity, which is at worst $\mathcal{O}(n^3)$, where n is the number of samples. This makes the algorithm less efficient for larger datasets, which are common in physical systems. Furthermore, it is not possible to sequentially add new points to the graph. By training it on a small fraction (0.01) of the full dataset, the results are shown in Figure \ref{fig:SC_results_1} and \ref{fig:SC_results_2}. Because prediction of cluster membership for the rest of the dataset is hard, the solution was to plot the cluster memberships as a scatter plot. Overall, spectral clustering deals well with the different-shaped clusters, as the main structure in the flow was captured. And some of the key dynamics are identified, notably the inertial (green) and viscous sublayers (orange) (see Fig. \ref{fig:SC_results_2}). However, the inertial sublayer here extends vertically into the region identified by the original code as the laminar inflow region. Moreover, a large cluster, which spans the previously identified transitional and laminar inflow region, is simply considered to have all terms active (except the wall-normal stress term). One other novelty is that the free-flow region was identified to have no active terms.

Therefore, though it provides a non-Euclidean based clustering algorithm, spectral clustering is not well suited for this case study, or at least the aims of the method proposed by Callaham et al. (2021)\cite{callaham2021learning}. It is computationally expensive, cannot accept new data points, and additionally requires one extra parameter to set (\texttt{n\_neighbours}) when using nearest neighbours to build the graph.


\subsection{K-Means}

The next algorithm used is the K-Means algorithm. Compared to spectral clustering, it is much less computationally expensive and is able to accept new data points. The main drawback is that it may not be best suited for some physical system. For example, in the case of the turbulent boundary layer, the clusters are not spherical. More importantly, a great majority of points are near the origin, and K-Means clustering will struggle to distinguish that group of points as multiple clusters. One workaround used here is to set a higher number of clusters to find, which will force the algorithm to identify multiple clusters in the cloud of points near the origin.

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/KMeans_CovMat_14.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/KMeans_Clustering_Space_14.png}
    \subcaption{}
  \end{minipage}

  \caption{Results of the K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the K-Means algorithm. \textbf{(b)} Plot of the clusters found by the K-Means algorithm.}
  \label{fig:KMeans_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/KMeans_balance_models_14_8.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/KMeans_spca_clustering_14_8.png}
      \subcaption{}
  \end{minipage}

  \caption{More results for the K-Means algorithm method. \textbf{(a)} Plot of the unique balance models found after applying SPCA. \textbf{(b)} Plot of the unique balance model clusters in physical space. Identified balance models include an inflow region (purple), a inertial sublayer and inflow region (orange), a transitional layer (green), and a region that spans the free-flow region and the viscous sublayer(blue).}
  \label{fig:KMeans_results_2}
\end{figure}

The results of the K-Means algorithm are shown in Figure \ref{fig:KMeans_results_1} and \ref{fig:KMeans_results_2}. Using the larger cluster number did help in identifying distinct regions; however, the dominant balances identified do differ significantly from the ones in the original code. Again, clusters with similar dominant balances between the two methods here extend into other regions that had been identified to have very different dominant balance regimes (e.g. orange cluster in Figure \ref{fig:KMeans_results_2}).

\subsection{Weighted K-Means}

One solution to K-Means relying solely on Euclidean distance is to apply a weight to the samples. This is done to give more importance to the samples that are closer to the origin, essentially spreading them apart. This should encourage the algorithm to identify multiple clusters in the cloud of points near the origin. The weights are set to depend on the distance to the origin as follows (see Fig. \ref{fig:WKmeans_weights}):

\begin{equation}
  w_i = 1 - (\text{tanh}^{2}(\frac{1}{2}|\vec{OX}|)) \text{, where } \vec{OX} \text{ is the vector from the origin to the sample} x_i
\end{equation}

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_CovMat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_clustering_space_6.png}
    \subcaption{}
  \end{minipage}

  \caption{Results of the Weighted K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Weighted K-Means algorithm. \textbf{(b)} Plot of the clusters found by the Weighted K-Means algorithm.}
  \label{fig:WKMeans_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_balance_models_6_7.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_spca_clustering_space_6_7.png}
      \subcaption{}
  \end{minipage}

  \caption{More results for the Weighted K-Means algorithm method. \textbf{(a)} Plot of the unique balance models found after applying SPCA. \textbf{(b)} Plot of the unique balance model clusters in physical space. Identified balance models include an inflow region (purple), an inertial sublayer and inflow region (orange), a transitional layer (green), a region that spans the free-flow region and the viscous sublayer(blue), and finally, a region that spans the viscous sublayer (red)}
  \label{fig:WKMeans_results_2}
\end{figure}

The results of the Weighted K-Means algorithm are shown in Figure \ref{fig:WKMeans_results}. The results are quite similar to standard K-Means; however, they were obtained by having the GMM find 6 clusters only. Once again, some of the key dominant balance regimes are identified, but their location sometimes differ from the original code.

\subsection{Summary}

Overall, the Gaussian Mixture Model clustering algorithm offers really good flexibility thanks to its small number of hyperparameters, and ability to predict cluster membership for new data points. Additionally, it is able to identify clusters of different shapes and sizes, which is important when differentiating different dominant balances close to the origin. Another possible path to explore could be a mixture of other distributions, e.g., Cauchy.

\newpage

\section{Other case studies}

Briefly, case studies other than the turbulent boundary layer were also tested. With the exception of the flow around a cylinder and the rotating detonation engine which were not tested due to the data generating files. These were written using the already written turbulent boundary layer case, and following the paper and its supplementary information \cite{callaham2021learning}. The choice of hyperparameter was purely based on trying to obtain results as close as possible to the ones in the paper. Results are shown in extra plots in the Appendix (see Fig. \ref{fig:burst_neur_results_1}, \ref{fig:burst_neur_results_2}, \ref{fig:Geos_bal_results_1}, \ref{fig:Geos_bal_results_2}, \ref{fig:Opt_Pul_results_1}, and \ref{fig:Opt_Pul_results_2}). Overall, the results are quite similar to the ones in the paper, with some key dynamics being identified in all cases. Differences are usually limited to one or two terms being considered active or inactive erroneously for a given balance model. In some cases, instructions were lacking in terms of how the data should be transformed to its equation space form, and some assumptions had to be made. For example, with the optical burst case, because of dealing with complex numbers, the results were obtained by using the real part of the data.

\section{Stability Assessment}

Though Callaham et al. (2021)\cite{callaham2021learning} presents a well-generalizable method for unsupervised identification of dominant balance regimes, there are still two hyperparameters to set: the number of clusters to find when using the GMM algorithm, and the alpha value for the SPCA algorithm. These can have a large impact on the results obtained. This section aims to explore the stability of the results obtained when changing these hyperparameters and discuss the implications of this when using the method for previously unstudied physical systems.

\subsection{Under different number of clusters set}

The first test carried out is to see how results differ when setting a different number of clusters. Ideally, the cluster number set should not matter greatly. With the exception of very small numbers and approaching the limit: $n_{clusters} \approx n_{samples}$,  the results should return to the same "true" or expected value by applying Sparse PCA and combining clusters with identical dominant balances. However, this is not guaranteed. This is becauses by combining clusters, one also combines the active terms of the 2 clusters. And thus inversely, by increasing the number of initial clusters, this could result in dividing clusters into smaller, unidentically balanced clusters.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_cluster_numbers_bal_mods.png}
  \caption{Plot of the obtained balance models for different initial cluster number. Reading top to bottom and left to right, the cluster numbers are 4 to 15}
  \label{fig:diff_clust_num_bal_mods}
\end{figure}

The algorithm was run for multiple cluster numbers, and the balance models were obtained for each case. These are all plotted in Figure \ref{fig:diff_clust_num_bal_mods}. It can be seen that the results are mostly stable, with a slow increase in the number of final unique balance models found (see Fig. \ref{fig:diff_clust_num_nmodels}), but with identical balances identified, except for low cluster numbers. Importantly, the relevant case of 6 clusters seems to be unique and changes quite a lot for the 5 and 7 clusters case.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{../Plots/Stab_Ass/different_cluster_numbers_nmodels.png}
  \caption{Plot of the number of unique balance models identified for different number of cluster numbers}
  \label{fig:diff_clust_num_nmodels}
\end{figure}

However, checking the actual clustering in space in Fig. \ref{fig:diif_clust_num_clustering} and comparing to the balance models obtained, it can be seen that having identified the same balance models does not necessarily translate to the exact same clustering in space.


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_9.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_14.png}
      \caption{}
  \end{subfigure}
  \caption{Final unique balance models identified when the cluster number was set to 9 (a) and 14 (b).}
  \label{fig:diif_clust_num_clustering}
\end{figure}

\subsection{Under different alpha values}

To test the stability of the algorithm under changes to the second parameter: $\alpha$, for the $\ell$-1 regularization of the SPCA problem, the same test is done. Taking the case where the cluster number was set to 6, the unique dominant balance models were obtained for values of $\alpha$ going from 0.1 to 17. This is a choice based on the SPCA residuals curve obtained for the 6 clusters scenario (see Fig. \ref{fig:spca_residuals}) which shows these values being in an area of acceptable trade-off between sparsity and accuracy.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{../Plots/BL/GMM_spca_residuals_6.png}
  \caption{Plot of the SPCA residuals for different values of $\alpha$. Recall, the residuals are computed as the $\ell$-2 norm of the inactive terms for all clusters}
  \label{fig:spca_residuals}
\end{figure}

\newpage

Looking at the results in Fig. \ref{fig:diff_alpha_bal_mods}, there is a lot more variation in the results obtained, with as expected a lot more active terms and less balance models for small $\alpha$ values and the inverse case for larger values.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_alpha_bal_mods.png}
  \caption{Plot of the obtained balance models for different $\ell$-1 regularization term factor, $\alpha$. Reading top to bottom and left to right, the values are 0.1, 0.2, 0.5, 1, 2, 5, 7, 8, 10, 12, 15, and 17}
  \label{fig:diff_alpha_bal_mods}
\end{figure}

However, for values close to 10, which was selected in the paper, there is good stability, with values from 10 to 15 yielding the same results. And checking that it is still the case in physical space (see Fig. \ref{fig:diif_alpha_clustering}) shows that though the dominant balance terms are slightly different, the general result stays the same.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_alpha_8.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_alpha_12.png}
      \caption{}
  \end{subfigure}
  \caption{Final unique balance models identified when $\alpha$ was set to 8 (a) and 12 (b).}
  \label{fig:diif_alpha_clustering}
\end{figure}

\subsection{Under different training set size}

Finally, the effect of the training set size on the results is tested (see Fig. \ref{fig:diff_train_frac_bal_mods}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_train_frac_bal_mods.png}
  \caption{Plot of the obtained balance models for different training set size (as a fraction of the full dataset). Reading top to bottom and left to right, the training set size fractions are 0.01, 0.02, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9. Except the first one, there are 2 results that reappear. They are plotted in space in Fig. \ref{fig:diff_train_frac_clustering}.}
  \label{fig:diff_train_frac_bal_mods}
\end{figure}

From these results and plotting the dominant balance models in space, there is a seemingly random instability to the clustering in space, as the inertial sublayer, which delineates the vertical extent of the boundary layer is sometimes not well sectioned (see Fig. \ref{fig:diff_train_frac_clustering}), and this happens even for low and high training set size but not all. When this occurs, the dominant balances change with, for example, the green region in Fig. \ref{fig:diff_train_frac_clustering}(b) now occupying most of the space and having dynamics that seem to combine those of the inertial sublayer and of the free flow region.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
    \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_frac_0.1.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/Stab_Ass/spca_clustering_space_training_set_frac_0.5.png}
      \caption{}
  \end{subfigure}

  \caption{Final unique balance models identified when the raining set size fraciton was set to 0.1 (a), and 0.5 (b)}
  \label{fig:diff_train_frac_clustering}
\end{figure}

\subsection{Discussion}

From the above results, though the Callaham et al. (2021)\cite{callaham2021learning} method made considerable effort to have a minimal amount of parameters, the 2 that must be set do have a considerable impact on the results. And in the absence of expert understanding which exists for the boundary layer case, it is not immediately clear what parameters must be selected as many are essentially valid.




\chapter{Elasto-inertial turbulence}

As part of this project, there was an opportunity to attempt using the unsupervised identification of dominant balance method on a much more complex polymer-laden flow, which exhibits a special type of turbulence named Elasto-Inertial Turbulence (EIT). This is a relatively recent topic in fluid dynamics, and there is an interest in seeing whether the Callaham et al. method could be used to identify unknown dominant balance regimes in this flow, hopefully shedding some light on the dynamics at play, particularly near the boundaries.

\section{Background}

EIT was introduced around 2013 in a paper by Samanta et al.\cite{Samanta2012eit}, describing this new type of turbulence occuring in visco-elastic fluids. These are polymer-laden flows, which are found in several industrial application where small amounts of polymers are added to liquids to reduce turbulent drag in pipelines. Samanta et al.\cite{Samanta2012eit} also explored the conditions under which EIT occurs, and found it did so at high shear rates, through suppressing and replacing Newtonian turbulence. It also demonstrated that EIT was dominated by elastic stresses, and that their balance with inertial stresses was key in determining the onset of EIT. More recent advances include the use of Direct Numerical Simulations (DNS) for a Finitely Extensible Nonlinear Elastic fluid under the Peterlin approximation (FENE-P) in the EIT regime\cite{beneitez2024multistability}. This is essentially simulating visco-elastic fluids in the conditions where EIT is found to occur. The first is a paper by Sid, Terrapon, and Dubief \cite{sid2018two} which confirmed the existence of two dimensional elasto-inertial instabilities and highlighted the role of small elastic scales in sustaining EIT. The second paper, by Dubief et al.\cite{dubief2022first}, used DNS and found a coherent structure in EIT, which sustain turbulence through a balance of elastic and inertial stresses, and hence maintain EIT. There has thus been significant progress in understanding EIT. It is first dominated by elastic stresses when there is high polymer stretch and shear rates. Second are the inertial stresses particularly in the onset of EIT, and the viscous stresses matter less \cite{Samanta2012eit, sid2018two, dubief2022first}. However, Dubief et al.\cite{dubief2022first} still leaves some questions on the dynamics and interactions within EIT's coherent structures unanswered. This is the aim of using the Callaham et al. method on DNS data of EIT\cite{beneitez2024multistability}.

\section{Methodology}

The data for this case study is obtained from a two-dimensional DNS of the FENE-P model. This model has the following governing equations:

\begin{equation}
  \begin{aligned}
    \partial_t \mathbf{u} + (\mathbf{u} \cdot \mathbf{\nabla}) \mathbf{u} + \mathbf{\nabla} p &= \frac{\beta}{Re} \Delta \mathbf{u} + \frac{1 - \beta}{Re} \mathbf{\nabla} \cdot \mathbf{T}(\mathbf{C}), \\
    \partial_t \mathbf{C} + (\mathbf{u} \cdot \mathbf{\nabla}) \mathbf{C} + \mathbf{T}(\mathbf{C}) &= \mathbf{C} \cdot \mathbf{\nabla} \mathbf{u} + (\mathbf{\nabla} \mathbf{u})^T \cdot \mathbf{C} + \frac{1}{Re \cdot Sc} \Delta \mathbf{C}, \\
    \mathbf{\nabla} \cdot \mathbf{u} &= 0,
  \end{aligned}
\end{equation}

where:

\begin{equation}
  \begin{aligned}
    \mathbf{T}(\mathbf{C}) = \frac{1}{\text{Wi}} \left(f(\text{tr}\mathbf{C})\mathbf{C} - \mathbf{I}\right), \\
    f(x) = \left(1 - \frac{x - 3}{L_{\text{max}}^2}\right)^{-1}
  \end{aligned}
\end{equation}

$\mathbf{T}(\mathbf{C})$ is the Polymer Stress Tensor, $\mathbf{u}$ is the velocity field, $\mathbf{C}$ is the conformation tensor, and $p$ is the pressure. This simulation was carried out in the context of the Beneitez et al. (2024) paper\cite{beneitez2024multistability}, and the data was obtained from the authors. In that study, the dimensionless parameters were set to $\beta = 0.9$, $Re = 1000$, $Sc = 500$, and $Wi = 50$, and $L_{max}$, the polymer extensibility was set to vary between [70, 130]. These simulations aimed to explore the dynamical connections betweem the arrowhead structure discovered in Dubief et al. (2022)\cite{dubief2022first} and EIT. Results showed there being 4 coexisting regimes (called attractors): a laminar state, a Steady Arrowhead Regime (SAR), EIT and a Chaotic Arrowhead Regime (CAR)(see Fig. \ref{fig:attractors}).

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{./EIT_regimes.png}
  \caption{Plot of the attractors found in the DNS of the FENE-P model. The Elasto-Inertial Turbulence (EIT) (top-left), the Chaotic Arrowhead Regime (CAR) (top-right), the laminar state (bottom-left), and the Steady Arrowhead Regime (SAR) (bottom-right). There were also some edge-states found in the simulations, which are found in between different basins of attraction in the state-space. Image from Beneitez et al. (2024)\cite{beneitez2024multistability}}
  \label{fig:attractors}
\end{figure}

For this case study, focus was consecrated to the CAR regime, obtaining a series of 512 by 512 images (snapshots) of the velocity components, pressure and its time variant gradient, and the conformation tensor's 4 components through time. With these variables, the terms can be computed. However, to avoid approaching this problem too bluntly with the full set of equation, only the streamwise component of the first equation of Eq. (6.1) is studied (see Eq. (6.3)). This simplifies the problem and still gives us the opportunity to test the usefulness of the Callaham et al. method in a more complex, less studied flow.

\begin{equation}
  \begin{aligned}
  \partial_{t}u + u u_{x} + v u_{y} + p_{x} &= \frac{\beta}{Re} (u_{xx} + u_{yy}) \\
  &\quad + \frac{1 - \beta}{Re} \left[ \left( \frac{1}{Wi} \left(1 - \frac{\text{tr} \mathbf{C} - 3}{L_{\text{max}}^{2}} \right)^{-1} C_{xx} - 1 \right)_{x} \right. \\
  &\hspace{2.5cm} + \left. \left( \frac{1}{Wi} \left(1 - \frac{\text{tr} \mathbf{C} - 3}{L_{\text{max}}^{2}} \right)^{-1} C_{xy} \right)_{y} \right]
  \end{aligned}
\end{equation}

Once the terms are computed, these are used as features for each point in space. As discussed in the Stability Assessment section, choosing the hyperparameters for this case study is uncertain. Following the ordering of the method, the number of clusters is first chosen. The reasoning behind this step was to get the covariance matrices of the clusters for an increasing number of clusters, and then choose the number of clusters for which clusters start to have repeated covariance matrices, therefore indicating that most of the different balance models have likely been identified.

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/GMM_cov_mat_5.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/GMM_cov_mat_7.png}
    \subcaption{}
  \end{minipage}

  \caption{Covariance matrices of the clusters found by the GMM algorithm for 5 (a) and 7 (b) clusters.}
  \label{fig:EIT_GMM_results_1}

\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/GMM_cov_mat_9.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/GMM_cov_mat_11.png}
    \subcaption{}
  \end{minipage}

  \caption{Covariance matrices of the clusters found by the GMM algorithm for 9 (a) and 11 (b) clusters.}
  \label{fig:EIT_GMM_results_2}

\end{figure}

The results of the GMM algorithm for the EIT case study are shown in Figures \ref{fig:EIT_GMM_results_1} and \ref{fig:EIT_GMM_results_2}. From these covariance matrices, we see that between 9 and 11 clusters, only repeats are obtained, with no clusters having new or at the least significantly different covariance matrices than in the 9 clusters case. So 9 clusters was the chosen number of clusters to find.

Choosing the value for $\alpha$ is a little more ambiguous. A more brute force approach is chosen, simply looking at the multiple results for a range of $\alpha$ values. The range itself is chosen to be in the trade-off region between sparsity and accuracy, as seen in the residuals plot for the 9 clusters case (see Fig. \ref{fig:EIT_spca_residuals}).


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{../Plots/EIT/SPCA_residuals_9.png}
  \caption{Plot of the SPCA residuals for different values of $\alpha$ for the EIT case. Recall, the residuals are computed as the $\ell$-2 norm of the inactive terms for all clusters}
  \label{fig:EIT_spca_residuals}
\end{figure}

Now based on the 9 covariance matrices (see Fig. \ref{fig:EIT_GMM_results_2}(a)), it would not be unreasonable to expect to find 4 to 7 unique balance models. Looking at the balance models found for the different alpha values in Fig. \ref{fig:EIT_diff_alpha},

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/EIT/different_alpha_bal_mods_zoom_9.png}
  \caption{Plot of the obtained balance models for different $\ell$-1 regularization term factor, $\alpha$. Reading top to bottom and left to right, the values are 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4}
  \label{fig:EIT_diff_alpha}
\end{figure}

Clearly there is a lot of variation between $\alpha =1$ and $\alpha = 2$, with the number of unique balance models decreasing from 8 to 4, and results for multiple $\alpha$ values will be studied and observed.

\newpage

\section{Results}

First, the data contained multiple snapshots and trajectories so the 15$^{\text{th}}$ snapshot of the 5$^{\text{th}}$ trajectory was chosen to be studied. For visualisation purposes, a plot of the first diagonal component of the comformation tensor, $C_{xx}$, is shown in Fig. \ref{fig:EIT_Cxx}. In that plot, the chaotic arrowhead structure is clearly visible.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/EIT/Cxx.png}
  \caption{Plot of the first diagonal component of the conformation tensor, $C_{xx}$, for the 15$^{\text{th}}$ snapshot of the 5$^{\text{th}}$ trajectory.}
  \label{fig:EIT_Cxx}
\end{figure}

Getting the terms for the streamwise component of the velocity equation did raise one issue, which was the value of the residuals of the equation. By putting all terms on the same side of the equation, the residuals were computed and plotted in Fig. \ref{fig:EIT_residuals}. As can be seen the residuals are overall low but the colormap shows that there some peak regions which reach order of magnitude 1. And compared to the terms' orders of magnitude ($1$, $10^{-1}$, $10^{-2}$), this is quite high. There are a couple of possible reasons for this, the first being the computing of the time derivative terms which relies on using a $2^{\text{nd}}$ order finite difference scheme, using the previous and next snapshots. This is a fairly strong assumption as one time step in these simulations are consequent. The second can be attributed to cumulative errors in the other derivatives. On the same note, the data ussed does have a spectral version, which could possibly have smaller cumulative errors when differentiating, so this could be a possible direction to explore in future work. Overall this does not necessarily invalidate our results but does mean that caution must be taken when interpreting the results.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/EIT/Equation Residuals.png}
  \caption{Plot of the residuals of the streamwise component of the FENE-P model equation for the EIT case study.}
  \label{fig:EIT_residuals}
\end{figure}


Having chosen the number of clusters to be 9, the clustering in space was obtained and is plotted in Fig. \ref{fig:EIT_clustering}. Though with the large number of clusters, it is hard to discern any visible patterns yet.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/EIT/GMM_clusters_9.png}
  \caption{Plot of the clustering in space for $n_{clusters}=9$ for the EIT case study.}
  \label{fig:EIT_clustering}
\end{figure}

Moving on to getting the balance models, Fig. \ref{fig:EIT_diff_alpha} already shows the balance models obtained for different $\alpha$ values. Recalling our expected number of balance models to be between 4 and 7, based on Fig. \ref{fig:EIT_GMM_results_2}, balance models in space were obtained for values of alpha between 1.25 and 2. The results are shown in Fig. \ref{fig:EIT_balance_models_space}. For clarity, the associated balance models are shown in Fig. \ref{fig:EIT_balance_models}.

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.75\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/spca_clustering_space_9_1.25.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.75\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/spca_clustering_space_9_1.5.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.75\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/spca_clustering_space_9_1.75.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.75\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/spca_clustering_space_9_2.0.png}
    \subcaption{}
  \end{minipage}

  \caption{Plot of the unique balance models in space found for $\alpha$ values of 1.25 (a), 1.5 (b), 1.75 (c), and 2 (d).}
  \label{fig:EIT_balance_models_space}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/balance_models_9_1.25.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/balance_models_9_1.5.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/balance_models_9_1.75.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/EIT/balance_models_9_2.0.png}
    \subcaption{}
  \end{minipage}

  \caption{Plot of the unique balance models found for $\alpha$ values of 1.25 (a), 1.5 (b), 1.75 (c), and 2 (d).}
  \label{fig:EIT_balance_models}
\end{figure}


\section{Discussion}

Overall, Fig. \ref{fig:EIT_balance_models_space} and \ref{fig:EIT_balance_models} show that there is a clear 2 or 3 groups of dominant balance regimes. The first are the clusters which have neither elastic or viscous stresses (pink, brown, purple, red except for $\alpha = 1.25$, and green). These occupy the majority of the channel, being primarily along the walls or the centre of the channel, with the exception of the where the arrowhead is. The second group are the orange and blue clusters which are associated with regions with strong elastic and viscous stresses, with the latter only being in the orange cluster. From literature \cite{beneitez2024multistability,Samanta2012eit,dubief2022first}, this fits with conditions for which EIT is maintained. In terms of location, elastic stresses are localised to the sides of the channel, and where the arrowhead structure is. The viscous stresses are more localised, only on the sides of the channels for x positions that are aligned with where the arrowhead structure stops.


\chapter{Conclusion}

Thus, this report has evaluated the reproducibility and robustness of the method proposed by Callaham et al. (2021). By reproducing results with alternative code and comparing them with the original code's results, this project has confirmed the validity of the original findings while uncovering some limitations and areas for improvement. By testing other clustering algorithms, the choice of the GMM algorithm was reinforced, demonstrating its effectiveness in identifying dominant balance regimes for a variety of datasets. Furthermore, the stability analysis provided valuable insights into the sensitivity of the method to hyperparameters, highlighting the importance of careful selection to ensure robust and reliable results.

The exploration of elasto-inertial turbulence (EIT) further highlighted the flexibility of the method, identifying plausible dominant balance regimes and providing insights into the dynamics of the flow. Despite encountering some challenges with code portability and data consistency, the overall method is simple to implement and can be applied to a wide range of physical systems.

The application of the method to new datasets suggests promising avenues for future research, particularly in extending the approach to other types of flows and refining the algorithm for better accuracy and efficiency. Ultimately, this work contributes to the broader effort of leveraging data-driven techniques to enhance our understanding and modeling of complex physical phenomena.


\chapter{Appendix}

\section{Algorithms}


\begin{definitionbox}{Algorithm 1: Gaussian Mixture Model clustering}
  \begin{algorithmic}[1]
    \State Data $\vec{x} \in \mathbb{R}^{n \times n_{features}}$, number of clusters/Gaussians to fit $K$ \Comment{Inputs}
    \State Cluster assignments $z \in \mathbb{R}^n$ \Comment{Output}
    \State $w_{k} \gets \frac{1}{k}$ \Comment{Initialisation of the weights}
    \State $\mathbf{\mu_{k}} \gets \mathbf{c_{k}}$ \Comment{Initialise the means as K-Means centroids}
    \State $\mathbf{\Sigma_{k}} \gets \mathbf{\Sigma}_{C_{k}}$ \Comment{Initialise as the covariance matrices of the clusters}
    \State $C_{k} \gets w_{k} \times \mathcal{N}(\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ \Comment{Initialise the clusters/Gaussians}

    \State \textbf{Expectation-Maximisation:}
    \State $\mathcal{L} \gets 0$ \Comment{Initialise the log-likelihood}
    \For{$i < \text{max-iter}$}
      \State \textbf{Expectation step:}
      \For{$k \in K$}
        \State $\mathbf{b_{k}} \gets \frac{w_{k} \times f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})}{\sum_{j=1}^{K} w_{j} \times \mathcal{N}(\mathbf{x}|\mathbf{\mu_{j}}, \mathbf{\Sigma_{j}})}$ \Comment{Compute the responsibilities for each cluster}
      \EndFor
      \State \textbf{Maximisation step:}
      \For{$k \in K$}
        \State $w_{k} \gets \frac{1}{n} \sum_{i=1}^{n} \mathbf{b}_{k}$ \Comment{Update the weights}
        \State $\mathbf{\mu_{k}} \gets (\sum \mathbf{b}_{k}\mathbf{x})/\sum \mathbf{b}_{k}$ \Comment{Update the means}
        \State $\mathbf{\Sigma_{k}} \gets (\sum \mathbf{b}_{k}(\mathbf{x} - \mathbf{\mu}_{k})^{2})/\sum \mathbf{b}_{k}$ \Comment{Update the covariance matrices}
      \EndFor
      \State $\mathcal{L}_{\text{new}} \gets \sum_{i=1}^{n} \log(\sum_{k=1}^{K} w_{k} \times f(x_{i}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}}))$ \Comment{Compute the log-likelihood}
      \State $\epsilon \gets \mathcal{L} - \mathcal{L}_{\text{new}}$ \Comment{Compute the difference in log-likelihood}
      \If{$\epsilon < 10^{-4}$}
        \State \textbf{break}
      \EndIf
      \State $\mathcal{L} \gets \mathcal{L}_{\text{new}}$ \Comment{Update the log-likelihood}
    \EndFor

  \end{algorithmic}
  Where:

  - $f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ is the probability density function of a multivariate normal distribution

  \label{alg:GMM}
\end{definitionbox}

\newpage

\begin{definitionbox}{Algorithm 2: Sparse PCA Algorithm}
  \begin{algorithmic}[1]
    \State Data matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ (n: samples, p: features), number of components $k$, sparsity controlling parameter $\alpha$, maximum iterations $\text{max\_iter}$, tolerance $\text{tol}$ \Comment{Inputs}
    \State Sparse components $\mathbf{V} \in \mathbb{R}^{p \times k}$ \Comment{Outputs}
    \State Initialize $\mathbf{V} \gets \text{random}$, $\mathbf{U} \gets \text{random}$ \Comment{Initialization}

    \State \textbf{Initialization:}
    \State Center the data matrix $\mathbf{X}$ by subtracting the mean of each column
    \State Initialize $\mathbf{V}$ with random values

    \For{$i = 1$ to $\text{max\_iter}$} \Comment{Iterate to optimize components and loadings}
      \State \textbf{Update Loadings:}
      \For{$j = 1$ to $k$} \Comment{Update each loading vector}
        \State $\mathbf{u}_j \gets \arg \min_{\mathbf{u} \in \mathbb{R}^n} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{u}_j\|_1$
      \EndFor

      \State \textbf{Update Components:}
      \For{$j = 1$ to $k$} \Comment{Update each component vector}
        \State $\mathbf{v}_j \gets \arg \min_{\mathbf{v} \in \mathbb{R}^p} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{v}_j\|_1$
      \EndFor

      \State \textbf{Convergence Check:}
      \State Compute the reconstruction error: $\text{error} \gets \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F$
      \If{$\text{error} < \text{tol}$}
        \State \textbf{break}
      \EndIf
    \EndFor

    \State \textbf{Return:} Sparse components $\mathbf{V}$

  \end{algorithmic}
  In practice, \texttt{sklearn} uses a dictionary learning for sparse coding\cite{mairal2010online}, and structured sparse PCA\cite{jenatton2010structured} for the sparse PCA algorithm. But both are attempting to solve the optimization problem: $(U^*, V^*) = \arg_{U, V} \text{min } \frac{1}{2} ||X-UV||_{\text{Fro}}^2+\alpha||V||_{1,1} \text{subject to } ||U_k||_2 <= 1 \text{ for all } 0 \leq k < n_{components}$
  \label{alg:SPCA}
\end{definitionbox}

\newpage

\begin{definitionbox}{Algorithm 3: Spectral Clustering Algorithm}
  \begin{algorithmic}[1]
    \State Given data points $\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n$ \Comment{Inputs}
    \State Construct similarity graph $G = (V, E)$ where each vertex $v_i$ represents a data point $\vec{x}_i$. \Comment{Graph construction}
    \State Choose a similarity function, here, k-nearest neighbours: Connect $v_{i}$ and $v_{j}$ if one of these is among the k-nearest neighbours of the other.

    \State \textbf{Step 1: Compute Laplacian}
    \State Construct the adjacency matrix $A$ where:

    $A_{ij} = \begin{cases} 1 & \text{if } {i,j} \in E\text{, the Edge set} \\0 & \text{otherwise} \end{cases}$
    \State Construct the degree matrix $D$ where $D_{ii} = \sum_{j} A_{ij}$
    \State Construct the unnormalized Laplacian $L = D - A$
    \State \textbf{or} construct the normalized Laplacian $L_{\text{sym}} = I - D^{-1/2} A D^{-1/2}$

    \State \textbf{Step 2: Compute Eigenvectors}
    \State Compute the first $k$ eigenvectors $u_1, u_2, \ldots, u_k$ of $L$ \textbf{or} $L_{\text{sym}}$
    \State Form the matrix $U \in \mathbb{R}^{n \times k}$ by stacking the eigenvectors in columns

    \State \textbf{Step 3: Cluster in embedding space}
    \State Apply k-means clustering to the rows of $U$ to obtain clusters $C_1, C_2, \ldots, C_k$

    \State \textbf{Output:} Clusters $A_1, A_2, \ldots, A_k$ where $A_i = \{ j | \vec{u}_j \in C_i \}$
  \end{algorithmic}
  This is for an unweighted undirected graph.\cite{luxburg2007tutorial}
  \label{alg:Spect_Clust}
\end{definitionbox}

\newpage

\section{Extra Plots}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../Plots/tanh_weights.png}
    \caption{Plot of the sample weight function applied to the turbulent boundary layer data when using the weighted K-measn algorithm. The function is a hyperbolic tangent function with a maximum value of 1 and a minimum value of 0. This is a function of the distance of the sample from the origin in equation space.}
    \label{fig:WKmeans_weights}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.9\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Burst_Neur/cov_mat_9.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Burst_Neur/final_clusters_9_110.png}
    \subcaption{}
  \end{minipage}

  \caption{Results for the Bursting R15 Neuron case. \textbf{(a)} Covariance matrices for each of the clusters found by the GMM algorithm. \textbf{(b)} Plot of the clusters found by the GMM algorithm in physical space. }
  \label{fig:burst_neur_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.8\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/Burst_Neur/cluster_9.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/Burst_Neur/final_balance_models_9_110.0.png}
      \subcaption{}
  \end{minipage}

  \caption{Second part of the results for the Bursting R15 Neuron case. \textbf{(a)} Plot of the clusters found by the GMM algorithm. \textbf{(b)} Plot of the unique balance models in t-V space found after applying SPCA. The spikes start with the voltage being balanced by sodium currents only in the rising part (orange), then sodium and potassium at the peak (green), and then with the slow inward Calcium current added (purple), which differs from the paper's results. Overall, the peak-cluster is too large and should stay in the positive mV region. The grey and blue clusters follow the same dynamics as in the paper, where mostly calcium dependence terms dominate.}
  \label{fig:burst_neur_results_2}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Geos_Bal/cov_mat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Geos_Bal/clusters_6.png}
    \subcaption{}
  \end{minipage}

  \caption{Results for the geostrophic balance in oceanic currents case. \textbf{(a)} Covariance matrices of for each of the clusters found by the GMM algorithm. \textbf{(b)} Plot of the clusters found by the GMM algorithm.}
  \label{fig:Geos_bal_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/Geos_Bal/balance_models_6_40.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Geos_Bal/balance_models_space_6_40.png}
    \subcaption{}
  \end{minipage}

  \caption{Second part of results for the geostrophic balance in oceanic currents. \textbf{(a)} Plot of the unique balance models found after applying SPCA. \textbf{(b)} Plot of the unique balance model clusters in physical space. Thus similar results to the paper's are obtained, with the geostrophic balance (blue) holding where there are vortices. The main difference with the paper is the number and size of these regions which is due to the fact that the snapshots used (in time and date) are not the same, so conditions were different..}
  \label{fig:Geos_bal_results_2}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Opt_Pul/cov_mat_6.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Opt_Pul/clustermap_6.png}
    \subcaption{}
  \end{minipage}

  \caption{Results for the optical pulse case. \textbf{(a)} Covariance matrices of for each of the clusters found by the GMM algorithm. \textbf{(b)} Plot of the clusters found by the GMM algorithm.}
  \label{fig:Opt_Pul_results_1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/Opt_Pul/balance_models_6_10.0.png}
      \subcaption{}
  \end{minipage}

  \begin{minipage}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/Opt_Pul/balance_models_3D_6_10.0.png}
    \subcaption{}
  \end{minipage}

  \caption{Second part of results for the optical pulse case. \textbf{(a)} Plot of the unique balance models found after applying SPCA. \textbf{(b)} Plot of the unique balance model clusters in 3D projection of physical space, with height given by the intensity of the solitons. Overall, similar dominant balances were found. The strongest soliton identified balance (red), which was found in the paper to be the only region where the instantaneous nonlinear response was identified as active, though with an extra active term (Raman term wihtout cubic Kerr nonlinearity). But it is still the only identified balance with just the nonlinearity term active.}
  \label{fig:Opt_Pul_results_2}
\end{figure}

\newpage

\section{AI Use Declaration}

For the coding of this project, the following AI tools were used:
\begin{enumerate}
  \item GitHub Copilot's autogeneration tool was used when writing the documentation and the comments for the code. It was also used in the case of repetitive parts of the code, taking the example of the stability assessment python file, where the same loop is essentially used 3 times but with a different varying parameter each time, or for the large plot of the terms fields in the EIT Notebook. It was also used extensively to write the bibtex references by giving it links or copy pasting the pages of articles.
  \item ChatGPT was used:
    \subitem - To generate a detailed plan with recommended word counts for the executive summary, and a list of recommended figures to include. The plan generated only allocated 50 words to the turbulent boundary layer reproducing section which was not enough so the plan was modified to allocate a lot more words to that section.
    \subitem - It was also used to provide summaries for the literature on EIT.
    \subitem - It was asked for recommendations on if any other library other than sklearn would have a SPCA implementation: It recommended SPAMS\cite{mairal2014spams}, by the same authors as the dictionary learning algorithm used in sklearn \cite{mairal2010online}. This was not used in the end as the documentation was not clear on how to use it, and it seemed to require 2 complexity parameters, which would have made it harder to use than sklearn's implementation, on top of losing the quality of having not many hyperparameters to tune. It also recommended the mlxtend library, however, for this it seemed to simply lack an implementation of SPCA.
    \subitem - Extensive use was made when debugging issues with the Containerisation of this project. Because of the packages installed in the environment such as clawpack, mlxtend or spams, there were a lot of conflict issues which were hard to resolve as Docker error messages can be a bit obscure.
    \subitem - It was used in the proof reading of the report, giving paragraphs of the report one at a time and asking it to give me where spelling mistakes were.
    \subitem - When trying to plot the balance models, which uses latex formulations, running this on the docker brought up a matplotlib error saying it failed to process a string because latex could not be found. ChatGPT suggested installing missing latex packages when building the docker image: "dvipng texlive-latex-extra texlive-fonts-recommended cm-super". This fixed the issue.
    \subitem - It was also used early on when trying to understand how the scipy.sparse library functionned, asking it to explain step by step how the sparse matrices were being built in the original Callaham et al. code.
    \subitem - In general, it was used often by simply giving it the traceback of an error, which would usually return an explanation of what the error was pointing to.

\end{enumerate}


\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
