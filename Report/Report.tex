\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{setspace} % Add the setspace package
\usepackage{titlesec} % Add the titlesec package for customizing titles

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{cleveref}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
  breakable,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}[1]{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=#1,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
  breakable,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}[1]{
  colback=green!5!white,    % Light green background
  colframe=green!75!black,   % Darker green frame
  colbacktitle=green!85!black,  % Even darker green for the title background
  title=#1,         % Title text for example box
  fonttitle=\bfseries,    % Bold title font
  enhanced,
  breakable,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\onehalfspacing

\geometry{top=1.5in} % Adjust the value as needed

% Customization for chapter titles
\titleformat{\chapter}[display] % Use 'display' to put number and title on separate lines
  {\normalfont\LARGE\bfseries} % Format for the chapter title
  {Chapter \thechapter} % Display "Chapter X"
  {0.5em} % Space between "Chapter X" and the title
  {\Huge} % Chapter title format
\titlespacing*{\chapter}{0pt}{-20pt}{20pt} % Adjust spacing around chapter title

% ----------------------------------------------------------------



\begin{document}

\begin{titlepage}
  \centering
  \vspace*{2cm}
  {\LARGE\bfseries MPhil DIS Report 24\par}
  \vspace{1cm}
  {\Large\itshape\ CRSiD:\ tmb76\par}
  \vspace{1cm}
  {\Large\itshape\ University of Cambridge\par}
  \vfill
  {\large\today\par}
\end{titlepage}

\tableofcontents

\chapter{Executive Summary}


\chapter{Introduction}


One of the key steps of scientific method is reproducibility. Results must be reproducible by others, ensuring that the same conclusions can be drawn multiple times. If a result cannot be reproduced, it may be considered erroneous, or simply a random occurence. This project therefore has for a core aim to evaluate the reproducibility of the results of the Callaham et al. (2021)\cite{callaham2021learning} paper and to test the robustness of their method.

\vspace{5mm}

For many problems in engineering and physical sciences, equations involve a large number of terms and complex differential equations. Simulating them can be computationally expensive or unnecessarily so, due to multiple asymptotic local behaviours where the system is dominated by a subset of the terms. In such cases, one can simplify the equations to a balance between these dominant terms, and simulate the system with sufficient accuracy and relatively lower computational cost \cite{charney1990scale}. This method, known as dominant balance or scale analysis, has been a powerful tool in physics.

\vspace{5mm}

And though extremely useful, dominant balance also requires expertise and is usually done by hand in time-consuming proofs. This report discusses and verifies a novel approach, developped by Callaham et al. (2021)\cite{callaham2021learning}, which explores using data from a physical system and machine learning methods to identify dominant balances algorithmically. First, this report will focus on the paper and the research surrounding it. Delving into what the rationale behind the method is, and how it performed on a series of case studies, as well as verifying it through reproducing the results with alternative code. This will be done primarily focusing on one of the case studies, but also for most of the others. Additionally, other algorithms than the method's chosen one are used to test the robustness of the method. Second, the method will be used on a new dataset, from simulations of elasto-inertial turbulence, a property of polymer laden flow.


\chapter{Background}


As aforementionned, dominant balance or scale analysis is a powerful tool in simplifying the modelling of physical processes. Importantly, it helps better understand the physics at play in a system by identifying the subset of terms that truly matter in an equation for a given asymptotic case. Simplifying the equations also leads to easier computations by avoiding unnecessary complications of the model. Taking the example of meteorology, where modelling the entire atmosphere from the full Navier-Stokes equations of motion would have an immense computational cost. And a large part of improvements in numerical weather predictions can be attributed to scale analysis \cite{charney1947dynamics, phillips1963geostrophic, burger1958scale, yano2009scale}.

\vspace{5mm}

However, this process can be slow as it requires considerable expertise from researchers. And for most of the well studied physical systems, this was done by hand a few decades ago. But with the wealth of computational power and data science techniques there is today, an attempt at automating dominant balance can be made. First is the Portwood et al. (2016)\cite{portwood2016robust} paper which used a cumulative distribution function on local density gradients to separate each region of a stratified turbulent flow. But the method used is highly tailored for its case study, as the gradient of one of the terms is used, knowing it has dynamics discerning qualities. Further, the results are interpreted through expert analysis of the identified regions. Second is the work carried out in Lee \& Zaki (2018)\cite{lee2018detection} where an algorithm to detect different dynamical regions is introduced. But again it is through the use of case-specific variables (vorticity), which restrict the use of this algorithm to certain flows. Finally, Sonnewald et al. (2019)\cite{sonnewald2019unsupervised} used a K-Means clustering algorithm to identify dynamically distinct regions in the ocean. And though they do introduce the concept of using the terms in the governing equations as features, the identification of active terms is done through comparison of the magnitudes of each term in the equation. In other words, identifciation of dominant terms is not done algorithmically but ``manually''. Thus, these methods are mostly designed for specific case studies and partly rely on expert knowledge to interpret the results.

\vspace{5mm}

A similar challenge in data science and machine learning has been to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009)\cite{schmidt2009distilling} contributed to a breakthrough using symbolic regression to find linear and non-linear differential equations. And this was improved in Brunton et al. (2016)\cite{brunton2016discovering}. As symbolic regression is expensive, the problem was approached with sparse regression, which for high-dimensional problems means identifying a sparse governing equation. The rationale behind this is that governing equations usually having only a subset of terms being important, as in dominant balance. Lejarza \& Baldea (2022) further advanced this by using multiple basis functions and a non-linear moving horizon optimization to learn governing equations from noisy data. Deep learning methods have also been used in this effort. First, where the lagrangians are learned, therefore learning how to model complex physical systems, and learning symmetries and conservation laws, where other networks failed \cite{cranmer2020discovering}. Second, deep learning (Graph Neural Network) and symbolic regression are combined to create a general framework to recover equations of physical systems \cite{cranmer2020lagrangian}. This method has the advantage of being generalisable and therefore useable to extract plausible governing equations for unknown systems.

\vspace{5mm}

This generalisable quality is precisely the gap that Callaham et al. (2021)\cite{callaham2021learning} attempt to fill in the identification of dominant balance models. They propose a novel approach to take in simulated or measured data from a physical system, and extract dominant balance models with minimal user input. This means one could use it in conjunction with the above governing equation identifying methods, and essentially automatically learn the governing equations and asymptotic regimes of that equation for any given physical system. This is a very powerful result however, as Schmidt \& Lipson (2009) noted for their work, this method should be seen as a guiding tool to help indicate where scientists should focus their attention, rather than a definitive answer. This is an important point which will be further discussed in this report.


\chapter{Methodology}

The method proposed by Callaham et al. (2021)\cite{callaham2021learning} can be summarized in three steps. First, representing the data in an equation space, with the terms as features. Second, using the Gaussian Mixture Model clustering algorithm to identify groups with a similar balance of terms. Finally, using Sparse Principal Component Analysis to identify the active terms in each cluster. This section will delve into the details of each of these steps, with a summary flow chart in Figure \ref{fig:method_flowchart}.

\section{Data \& Equation Space Reprensentation}

As previously mentionned, obtaining the data can be done through simulations or measurements, with almost no restrictions as to what equations can be studied. The data comes in the form of space-time fields of physical variables: $u(\vec{x}, t)$, where $u$ is the physical variable considered (e.g. $\vec{u}$, $p$, etc. for the Navier-Stokes equations). The variables must be sufficient so that one may derive all the terms of the equation from them. One important requirement is that the computed terms must all balance out to 0 for each point in time and space. This comes down to how one computes said terms. They must be as they are in the equation, since this results in a linear covariance structure, as each term is balanced by a linear combination of the other terms\cite[Supplementary Information]{callaham2021learning}. For example, if a term is a squared comnbination of variables, it must be computed as such, and not as the combination of variables.

\vspace{5mm}

The fundamental idea of the Callaham et al. (2021) method is to take these physical-space fields of terms and organize them into an equation space where each dimension is one of the terms in the equation, and each sample is a point in space and time. Thus, each sample will be a vector $\vec{f} \in \mathbb{R}^k$ where $k$ is the number of terms in the equation, such that:

\begin{equation}
    \vec{f} = \begin{bmatrix} f_1(u(\vec{x}, t), \hdots) \\ f_2(u(\vec{x}, t), \hdots) \\ \vdots \\ f_k(u(\vec{x}, t), \hdots) \end{bmatrix}
\end{equation}

Where $f_i$ is the $i^{th}$ term in the equation, itself a function of the physical variables. Again, one must ensure that for each sample, the terms all balance out to 0, or that the residual is several orders of magnitude smaller than the terms themselves. This is in order to make sure that the equation studied or data used is not invalid.


\section{Gaussian Mixture Model Clustering}

Geometrically, by clustering points in this equation space, one can identify groups of points that have a similar balance of terms. Here, the chosen algorithm is the Gaussian Mixture Model (GMM) clustering algorithm. This algorithm relies on the assumption that the data has been generated from a mixture of a finite number of Gaussian distributions with unknown parameters\cite{mit2015algorithmic}. It has the advantage of being able to identify clusters with varying shapes and sizes. It only requires one hyperparameter, which is the number of clusters one wants the algorithm to find, and therefore how many Gaussian distributions the data is assumed to have been generated from\cite{sklearnGMM}. Thus, for each case study, a number of clusters to identify needs to be chosen. Generally, the choice should be conservative, aiming for a number of clusters that is likely to be greater than the true/expected number of dominant balance regimes in the system.

\vspace{5mm}

The way GMMs fit to the data is by using the Expectation-Maximisation algorithm. This algorithm starts from an initial guess for the parameters of the gaussian distributions. The expectation step then evaluates the sum posterior probability of each point belonging to each cluster. And the maximisation step updates the parameters with weighted averages of those posterior probabilities\cite{dempster1977maximum} (see Alg. 1, in section \ref{alg:GMM}). This converges to the maximum likelihood estimates of the parameters (see Fig. \ref{fig:GMM_example}). Once fitted, cluster membership of new data points can be determined by taking the cluster with the highest probability at that point.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{./GMMexample 1.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 2.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 3.png}
  \includegraphics[width=0.4\textwidth]{./GMM example 4.png}
  \caption{Example of a Gaussian Mixture Model fit to data. The data is generated from 3 Gaussian distributions, and the GMM algorithm fits 3 Gaussians to it. \cite[1D Example Notebook]{gmm_towardsdatascience}}
  \label{fig:GMM_example}
\end{figure}

Another key output of the GMM is in its probabilistic nature which has the covariance matrix of each cluster/Gaussian as part of its output, thus describing the covariance between each term in the equation within that cluster. This gives insight into which terms could be active in each cluster, and also helps illustrate how the method can identify active terms geometrically. If only a subset of the terms in a cluster have non-zero covariance, then it means that it is only along that subset of dimensions that the cluster varies, and hence the terms are non-negligible.

\section{Sparse Principal Component Analysis}

With the points grouped in clusters of similar balance of terms, the next step is to identify which terms are active and which can be dropped to simplify the equation for that cluster. This is done through Sparse Principal Component Analysis (SPCA). Principal Component Analysis (PCA) is a method that is usually used to reduce the dimensionality of the data, whilst maximising the information retained. The new dimensions onto which the data is projected are called principal components and are the directions in which the data has the greatest variance\cite{lever2017principal}. SPCA differs from PCA in that it adds a sparsity constraint to the principal components, resulting in most coefficients of the PCs being zero. This is done by adding a L1 penalty to the objective function of PCA for the number of non-zero coefficients, leading to some coefficients being shrunk down to 0\cite{zou2006sparse} (see Alg. 2, in section \ref{alg:SPCA}).


\chapter{Conducted research}

To test and validate this method, Callaham et al.\ used a series of case studies\cite{callaham2021learning}. While some were well-studied physical systems with well-known dominant balance regimes (e.g., turbulent boundary layer, geostrophic balance currents), others were more complex and less understood (e.g., optical pulse generation, rotating detonation engine). For these latter cases, the paper presents a first plausible identification of dominant balance regimes. In this chapter, the aim is to discuss the reproducibility of the results presented in the paper, as well as to test and discuss the limitations of the method.

\section{Portability of the code}

One great quality of Callaham et al. (2021)\cite{callaham2021learning} is the sharing of their code in the form of runnable notebooks. With the overarching motivation for this project being the importance of reproducibility in today’s code-rich research environment, it is key that scientists share their code so one may verify how the results were obtained. This is also a great asset as it allows for a more useful reproduction of the results, where explicitly different results can be tested.

Overall, the code is of great quality, though some portability issues were encountered. Some of the dependencies were not stated in their README. More importantly, it seemed some of the data-generating code was modified between running their final notebook versions and their uploading to the repository. This led to some troubleshooting to get the data to match the data used in the paper. For the flow past a cylinder case, a file needed in the setup of the Nek5000 simulation software was missing\cite{nek5000setup}. For the bursting neuron case, the times for which the data was generated was wrongly set. Similarly, for the geostrophic current data, the remote reading code was selecting snapshots of the data that did not match the ones used in the paper, and the first 45 were set to zero, which had no real use. Further, the results in the paper are actually obtained from a different snapshot than the one stated in the paper. For this report, the data was simply downloaded from the HYCOM database \cite{hycom}, and the time was set to the same as the one used in the paper.

Nevertheless, the code was otherwise easy to run, and the notebook format was very useful for following the logic of the code. From this notebook, the code was first written for the turbulent boundary layer case. Explicitly alternative code was then written to test the reproducibility of the results.

\section{Reproducing of the results}

Reiterating previously made points, one of the main goals of this project is to verify the results of Callaham et al. (2021)\cite{callaham2021learning}. The objective is not only to check that the results can be reproduced but to do so using alternative code. The aim here is to have code written differently that, in practice, performs the same functions. This is to ensure that none of the main results from Callaham et al. are due to a “lucky” bug in their code, and were obtained through random chance.

\subsection{Turbulent Boundary Layer}

Because the underlying method is essentially the same for all case studies, the choice was made to put particular focus on just one of the cases, and less so on the others. The scenario chosen was the boundary layer in transition to turbulence.

Throughout the Callaham et al. (2021)\cite{callaham2021learning} notebooks, aside from the three main steps of the method, the large majority of the code involves handling the data, switching from equation space to physical space, and obtaining the unique balance models. Most of it is written using \texttt{Numpy}, which is the most efficient way. In the alternative code, however, \texttt{Pandas} was primarily used. In terms of performance, it is close to \texttt{Numpy}, especially for the size of the data in this study. For some code sections, however, impractical workarounds were required, particularly to replace the \texttt{numpy.unique} method.

The data for this case study is obtained from the John Hopkins database, which conducted a direct numerical simulation of a boundary layer over a stationary plate\cite{jhtdb}. This database provides the $x$ and $y$ coordinates as well as the following variables: $u$, the streamwise component of velocity;  $v$, the wall-normal component of velocity;  $p$, the fluid pressure; and $(\overline{u{\prime} v{\prime}})$ and $(\overline{u{\prime}^2})$, the wall-normal and streamwise Reynolds stress terms, respectively. From these variables, the terms of the streamwise component of the Reynolds-Averaged Navier-Stokes (RANS) equations need to be calculated:

\begin{equation}
  \bar{u} \bar{u}_x + \bar{v} \bar{u}_y = \rho^{-1} \bar{p}_x + \nu \nabla^2 \bar{u}  - (\overline{u' v'})_y - (\overline{u'^2})_x
\end{equation}

To compute the derivative terms, the method employed by Callaham et al. uses \texttt{scipy.sparse}’s sparse matrices. These sparse matrices are constructed so that when computing the matrix product with the fields or variables, the second-order forward/central/backward difference derivative is obtained, following the method described in ‘Fundamentals of Numerical Computations’\cite{finitediff}. Though it has the advantage of being portable for all variables, it is much slower than using the \texttt{numpy.gradient} function, which performs the same computation more explicitly when setting the \texttt{edge\_order} argument to 2. With the derivatives computed, spatial fields of each term in the equation can be plotted (see Fig. \ref{fig:RANS_terms}).

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using the original Callaham et al. code}
  \label{fig:RANS_terms}
\end{figure}

The next step is to use a Gaussian Mixture Model (GMM) to cluster the data in equation space. Callaham et al. judiciously chose to use the \texttt{sklearn} library’s implementation of the GMM algorithm. This implementation is likely very efficient, being written and optimized by experienced developers. With the chosen arguments, the algorithm is initialized using the \texttt{‘k-means++’} method. This method initializes the means of the Gaussians using the centroids found by the K-Means clustering algorithm (see Algorithm 3), which is itself initialized based on the empirical probability distributions of the data\cite{arthur2007kmeans}. The covariance matrices are then initialized as the covariance matrices of the clusters found by the K-Means algorithm. The algorithm then uses the Expectation-Maximisation algorithm as expected. Convergence is evaluated using the log-probability of the data:

\begin{equation}
  \log \mathcal{L}(\vec{X} | \vec{\theta}) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} w_k \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \vec{\Sigma}_k) \right)
\end{equation}

The algorithm stops when the difference between the new and old log-likelihood is less than $10^{-3}$.

The alternative code was thus written to follow the same initialization, explicitly using \texttt{sklearn}’s K-Means algorithm with \texttt{‘k-means++’} initialization. The Expectation-Maximization algorithm is then implemented as described in Algorithm 1, using the total log-likelihood difference as the convergence criterion.

For Sparse Principal Component Analysis (SPCA), the \texttt{sklearn} library’s implementation is used. The method employed is based on an extension of sparse PCA, which utilizes structured regularization to constrain the sparsity patterns.

With SPCA completed, the active terms can be identified. From this point, the original and alternative code only differ in which library they predominantly use. The original code primarily uses \texttt{Numpy}, while the alternative code uses \texttt{Pandas}, which sometimes has similar functions and sometimes requires workarounds.


\subsection{Results}

The first result to check is the terms obtained in Fig. \ref{fig:RANS_terms}. These were obtained with the original code, which made use of \texttt{scipy.sparse}. The alternative code, which used \texttt{numpy.gradient} instead, gave the same results (see Fig. \ref{fig:custom_RANS_terms}). The reason Callaham et al. likely used \texttt{scipy.sparse} is because it is more generalizable to all variables; however, they also used \texttt{numpy.gradient} for other case studies. Using \texttt{numpy.gradient} also leads to considerable speedups.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/custom_RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using an alternative code}
  \label{fig:custom_RANS_terms}
\end{figure}

\newpage

The next step is to cluster the data in equation space. The resulting clusters’ covariance matrices were obtained and are plotted in Figure \ref{fig:GMM_cov_mat}. Plotting the current clustering in physical space gives the results in Figure \ref{fig:GMM_clusters}. As can be seen, the results do differ, with the alternative code needing 7 clusters to identify the transitional layer (instead of 6 for the original code). The difference could be explained by either the different convergence criteria or some of the covariance matrix regularizations performed in the \texttt{sklearn} implementation\cite{sklearnGMM}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \caption{Covariance matrices of for each of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm}
  \label{fig:GMM_cov_mat}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm. The cluster numbers here match the ones in Figure \ref{fig:GMM_cov_mat}, but not 0 indexed}
  \label{fig:GMM_clusters}
\end{figure}

\newpage

Then comes applying SPCA to each cluster to identify which terms are active in it. In this case, the \texttt{sklearn} function is still used, though with alternative code to handle the results. Due to the different clustering obtained, the optimal alpha values had to be different for each method. For the original code, the optimal alpha value was set to 10, but had to be set to 7 for the alternative code, in order to keep the inertial sublayer unique and separate from the background free-stream (see Fig. \ref{fig:balance_model_clusters}). The results of the SPCA algorithm are $n_{clusters}$  sets of active terms. In the case where some clusters have the same active terms, they are combined. This results in a set of unique balance models, which are plotted in Figure \ref{fig:balance_models}. Similarly to Figure \ref{fig:custom_GMM_cov_mat}, some clusters are well identified by both methods. In both methods, the spatial arrangements of the clusters are the same, though their dynamics sometimes differ, or even switch (free flow vs. inertial sublayer in orange). Overall, similar key dynamics are identified in most of the clusters. For example, the viscous sublayer in blue and green has the viscous forces and streamwise Reynolds’ stress terms as active. The inflow region crucially has the streamwise Reynolds’ stress term as active. And in both cases, the transitional layer is identical.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_balance_models.png}
      \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_balance_models.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance models found after applying SPCA, when using the original Callaham et al. code (a) and an alternative code (b). Here, the cluster colors will match the ones in Figure \ref{fig:balance_model_clusters}}
  \label{fig:balance_models}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_spca_clustering_space.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_spca_clustering_space.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the unique balance model clusters in physical space, when using the original Callaham et al. code (a) and an alternative code (b). \textbf{(a)} Identified balance models include a laminar inflow region (red), a free-stream region (green), an inertial sublayer (orange), a transitional layer (purple), and a viscous sublayer (blue). \textbf{(b)} Identified balance models include an inflow region with low Reynolds' stress (brown), a free-stream region (orange), an inertial sublayer with stream-wise and wall normal inertial forces (red), a transitional layer (purple), and a viscous sublayer (green and blue). The cluster colors here match the ones in Figure \ref{fig:balance_models}}
  \label{fig:balance_model_clusters}
\end{figure}

Overall, there is a good agreement between the results, considering differences in the specifics of the Gaussian Mixture Modelling clustering algorithm. The alternative code was able to reproduce the identification of important dynamics in the boundary layer. It is also important to note that, though the overall method presented in Callaham et al. (2021)\cite{callaham2021learning} brings large improvements in terms of generalizing the identification of dominant balance regimes for different cases, there are still two parameters to set: the number of clusters to find when using the GMM algorithm, and the alpha value for the SPCA algorithm. These parameters can have a large impact on the results obtained. This is a key point to keep in mind and will be further discussed in the Stability Assessment section.


\section{Exploration of other algorithms}

In the interest of further testing Callaham et al.’s method, and following the Supplementary Discussion in the Supplementary Information of the paper\cite{callaham2021learning}, other clustering algorithms could be used compared to Gaussian Mixture Models. This section explores a few options and discusses the results obtained.

\subsection{Spectral clustering}

Because of the nature of the data being dealt with, the similarity measure used by the clustering algorithm being Euclidean may not be the most appropriate. As a result, spectral clustering is a convincing candidate for this, and it was also suggested in the Supplementary Discussion of the paper\cite[Supplementary Information]{callaham2021learning}.

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/SC_CovMat.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/SC_clustering_space.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/SC_balance_models.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/SC_spca_clustering_space.png}
      \subcaption{}
  \end{minipage}

  \caption{Results of the Spectral Clustering algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Spectral Clustering algorithm. \textbf{(b)} Plot of the clusters found by the Spectral Clustering algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include....}

  \label{fig:SC_results}
\end{figure}

Using spectral clustering, the main drawback is the computational complexity $\mathcal{O}(n^3)$, where $n$ is the number of samples. Thus, it is not the most efficient algorithm for larger datasets, which most physical systems likely will be. Furthermore, it is not possible to sequentially add new points to the graph. Training it on a fraction of 0.01 of the full dataset, the results are shown in Figure \ref{fig:SC_results}. Because it is hard to predict cluster membership for all points, the solution was to plot the cluster memberships as a scatter plot. Overall, spectral clustering deals well with the different-shaped clusters, as it can be seen that the same main structure in the flow was captured. However, looking at the dynamics identified, it is once again the case that some of the key dynamics are identified, notably the inertial and viscous sublayers (orange and green). However, the inertial sublayer here extends vertically into the region identified by the original code as the laminar inflow region. Furthermore, a large cluster, which covers the previously identified transitional and laminar inflow region, is simply considered to have all terms active (except the wall-normal stress term).

Therefore, though it provides a non-Euclidean based clustering algorithm, spectral clustering is not the most appropriate for this case study, or at least the aims of the method proposed by Callaham et al. (2021)\cite{callaham2021learning}. It is computationally expensive, cannot accept new data points, and additionally can require one extra parameter to set (\texttt{n\_neighbours}) when using nearest neighbors to build the graph.


\subsection{K-Means}

The next algorithm used is the K-Means algorithm. Compared to spectral clustering, it is much less computationally expensive and is able to accept new data points. The main drawback is that it may not be best suited for any physical system. For example, in the case of the turbulent boundary layer, the clusters are not spherical. More importantly, a great majority of points are near the origin, and K-Means clustering will struggle to distinguish that group of points as multiple clusters. One workaround used here is to set a higher number of clusters to find, which may force the algorithm to identify the different clusters in the cloud of points near the origin.

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/KMeans_CovMat.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/KMeans_Clustering_space.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/KMeans_balance_models.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/KMeans_spca_clustering.png}
      \subcaption{}
  \end{minipage}
  \caption{Results of the K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the K-Means algorithm. \textbf{(b)} Plot of the clusters found by the K-Means algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include....}

  \label{fig:KMeans_results}
\end{figure}

The results of the K-Means algorithm are shown in Figure \ref{fig:KMeans_results}. Using the larger cluster number did help in identifying distinct regions; however, the dominant balance regimes identified in them do differ significantly from the ones identified by the original code. Again, clusters with similar dominant balances to those originally identified extend into other regions that had been identified to have very different dominant balance regimes (orange cluster in Figure \ref{fig:KMeans_results}).

\subsection{Weighted K-Means}

One solution to K-Means relying solely on Euclidean distance is to apply a weight to the samples. This is done to give more importance to the samples that are closer to the origin, essentially mimicking the effect of spreading them apart. This should encourage the algorithm to identify multiple clusters in the cloud of points near the origin. The weights are set as follows:

\begin{equation}
  w_i = 1 - (\text{tanh}^{2}(\frac{1}{2}|\vec{OX}|)) \text{, where } \vec{OX} \text{ is the vector from the origin to the sample} x_i
\end{equation}

\begin{figure}[htbp]
  \centering

  \begin{minipage}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_CovMat.png}
    \subcaption{}
  \end{minipage}

  \begin{minipage}{\textwidth}
      \begin{minipage}[b]{0.6\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_clustering_cpace.png}
          \subcaption{}
      \end{minipage}
      \begin{minipage}[b]{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_balance_models.png}
          \subcaption{}
      \end{minipage}
  \end{minipage}

  \begin{minipage}{0.7\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../Plots/BL/WKMeans_spca_clustering_space.png}
      \subcaption{}
  \end{minipage}
  \caption{Results of the Weighted K-Means algorithm for the turbulent boundary layer case. \textbf{(a)} Covariance matrices of for each of the clusters found by the Weighted K-Means algorithm. \textbf{(b)} Plot of the clusters found by the Weighted K-Means algorithm. \textbf{(c)} Plot of the unique balance models found after applying SPCA. \textbf{(d)} Plot of the unique balance model clusters in physical space. Identified balance models include....}

  \label{fig:WKMeans_results}
\end{figure}

The results of the Weighted K-Means algorithm are shown in Figure \ref{fig:WKMeans_results}. The results are quite similar to standard K-Means; however, they were obtained by having the GMM find 6 clusters only. Once again, some of the key dominant balance regimes are identified, but they are not assigned to the same locations as in the original code.

\subsection{Summary}

Overall, the Gaussian Mixture Model clustering algorithm offers really good flexibility thanks to its small number of hyperparameters, and for adding new data points. Additionally, it is able to identify clusters of different shapes and sizes, which is important when differentiating different dominant balances close to the origin. Another possible path to explore could be a mixture of other distributions, e.g., Cauchy.

\section{Stability Assessment}

Though Callaham et al. (2021)\cite{callaham2021learning} presents a well-generalizable method for unsupervised identification of dominant balance regimes, there are still two hyperparameters to set: the number of clusters to find when using the GMM algorithm, and the alpha value for the SPCA algorithm. These can have a large impact on the results obtained. This section aims to explore the stability of the results obtained when changing these hyperparameters and discuss the implications of this when using the method for previously unstudied physical systems.

\subsection{Under different number of clusters set}

The first test that can be carried out is to see how results differ when setting a different number of clusters. Ideally, the cluster number set should not matter greatly. With the exception of very small numbers and approaching the limit: $n_{clusters} \approx n_{samples}$,  the results should be similar thanks to applying Sparse PCA and combining clusters with identical dominant balances. However, this is not guaranteed. If one were to combine two clusters with 2 terms active, and one shared active term, the resulting cluster would have the 3 terms active. Inversely, by increasing the number of initial clusters, this could result in dividing clusters into smaller, unidentically balanced clusters.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../Plots/Stab_Ass/different_cluster_numbers_bal_mods.png}
  \caption{Plot of the obtained balance models for different initial cluster number}
  \label{fig:diff_clust_num_bal_mods}
\end{figure}

The algorithm was run for multiple cluster numbers, and the balance models were obtained for each case. These are all plotted in Figure \ref{fig:diff_clust_num_bal_mods}. It can be seen that the results are mostly stable, with a slow increase in the number of final unique balance models found. However, checking the actual clustering in space and comparing to the balance models obtained…

\subsection{Under different alpha values}



\subsection{Under different training set size}

\subsection{discussion}

\chapter{Elasto-inertial turbulence}

\section{Background}

\section{Methodology}

\section{Results}

\section{Discussion}

\chapter{Conclusion}


\chapter{Appendix}

\section{Algorithms}


\begin{definitionbox}{Algorithm 1: Gaussian Mixture Model clustering}
  \begin{algorithmic}[1]
    \State Data $\vec{x} \in \mathbb{R}^{n \times n_{features}}$, number of clusters/Gaussians to fit $K$ \Comment{Inputs}
    \State Cluster assignments $z \in \mathbb{R}^n$ \Comment{Output}
    \State $w_{k} \gets \frac{1}{k}$ \Comment{Initialisation of the weights}
    \State $\mathbf{\mu_{k}} \gets \mathbf{c_{k}}$ \Comment{Initialise the means as K-Means centroids}
    \State $\mathbf{\Sigma_{k}} \gets \mathbf{\Sigma}_{C_{k}}$ \Comment{Initialise as the covariance matrices of the clusters}
    \State $C_{k} \gets w_{k} \times \mathcal{N}(\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ \Comment{Initialise the clusters/Gaussians}

    \State \textbf{Expectation-Maximisation:}
    \State $\mathcal{L} \gets 0$ \Comment{Initialise the log-likelihood}
    \For{$i < \text{max-iter}$}
      \State \textbf{Expectation step:}
      \For{$k \in K$}
        \State $\mathbf{b_{k}} \gets \frac{w_{k} \times f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})}{\sum_{j=1}^{K} w_{j} \times \mathcal{N}(\mathbf{x}|\mathbf{\mu_{j}}, \mathbf{\Sigma_{j}})}$ \Comment{Compute the responsibilities for each cluster}
      \EndFor
      \State \textbf{Maximisation step:}
      \For{$k \in K$}
        \State $w_{k} \gets \frac{1}{n} \sum_{i=1}^{n} \mathbf{b}_{k}$ \Comment{Update the weights}
        \State $\mathbf{\mu_{k}} \gets (\sum \mathbf{b}_{k}\mathbf{x})/\sum \mathbf{b}_{k}$ \Comment{Update the means}
        \State $\mathbf{\Sigma_{k}} \gets (\sum \mathbf{b}_{k}(\mathbf{x} - \mathbf{\mu}_{k})^{2})/\sum \mathbf{b}_{k}$ \Comment{Update the covariance matrices}
      \EndFor
      \State $\mathcal{L}_{\text{new}} \gets \sum_{i=1}^{n} \log(\sum_{k=1}^{K} w_{k} \times f(x_{i}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}}))$ \Comment{Compute the log-likelihood}
      \State $\epsilon \gets \mathcal{L} - \mathcal{L}_{\text{new}}$ \Comment{Compute the difference in log-likelihood}
      \If{$\epsilon < 10^{-4}$}
        \State \textbf{break}
      \EndIf
      \State $\mathcal{L} \gets \mathcal{L}_{\text{new}}$ \Comment{Update the log-likelihood}
    \EndFor

  \end{algorithmic}
  Where:

  - $f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ is the probability density function of a multivariate normal distribution

  \label{alg:GMM}
\end{definitionbox}

\newpage

\begin{definitionbox}{Algorithm 2: Sparse PCA Algorithm}
  \begin{algorithmic}[1]
    \State Data matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ (n: samples, p: features), number of components $k$, sparsity controlling parameter $\alpha$, maximum iterations $\text{max\_iter}$, tolerance $\text{tol}$ \Comment{Inputs}
    \State Sparse components $\mathbf{V} \in \mathbb{R}^{p \times k}$ \Comment{Outputs}
    \State Initialize $\mathbf{V} \gets \text{random}$, $\mathbf{U} \gets \text{random}$ \Comment{Initialization}

    \State \textbf{Initialization:}
    \State Center the data matrix $\mathbf{X}$ by subtracting the mean of each column
    \State Initialize $\mathbf{V}$ with random values

    \For{$i = 1$ to $\text{max\_iter}$} \Comment{Iterate to optimize components and loadings}
      \State \textbf{Update Loadings:}
      \For{$j = 1$ to $k$} \Comment{Update each loading vector}
        \State $\mathbf{u}_j \gets \arg \min_{\mathbf{u} \in \mathbb{R}^n} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{u}_j\|_1$
      \EndFor

      \State \textbf{Update Components:}
      \For{$j = 1$ to $k$} \Comment{Update each component vector}
        \State $\mathbf{v}_j \gets \arg \min_{\mathbf{v} \in \mathbb{R}^p} \frac{1}{2} \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F^2 + \alpha \|\mathbf{v}_j\|_1$
      \EndFor

      \State \textbf{Convergence Check:}
      \State Compute the reconstruction error: $\text{error} \gets \|\mathbf{X} - \mathbf{U} \mathbf{V}^T\|_F$
      \If{$\text{error} < \text{tol}$}
        \State \textbf{break}
      \EndIf
    \EndFor

    \State \textbf{Return:} Sparse components $\mathbf{V}$

  \end{algorithmic}
  In practice, \texttt{sklearn} uses a dictionary learning for sparse coding\cite{mairal2010online}, and structured sparse PCA\cite{jenatton2010structured} for the sparse PCA algorithm. But both are attempting to solve the optimization problem: $(U^*, V^*) = \arg_{U, V} \text{min } \frac{1}{2} ||X-UV||_{\text{Fro}}^2+\alpha||V||_{1,1} \text{subject to } ||U_k||_2 <= 1 \text{ for all } 0 \leq k < n_{components}$
  \label{alg:SPCA}
\end{definitionbox}


\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
