\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{setspace} % Add the setspace package
\usepackage{titlesec} % Add the titlesec package for customizing titles

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{cleveref}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
  breakable,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}[1]{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=#1,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
  breakable,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}[1]{
  colback=green!5!white,    % Light green background
  colframe=green!75!black,   % Darker green frame
  colbacktitle=green!85!black,  % Even darker green for the title background
  title=#1,         % Title text for example box
  fonttitle=\bfseries,    % Bold title font
  enhanced,
  breakable,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\onehalfspacing

\geometry{top=1.5in} % Adjust the value as needed

% Customization for chapter titles
\titleformat{\chapter}[display] % Use 'display' to put number and title on separate lines
  {\normalfont\LARGE\bfseries} % Format for the chapter title
  {Chapter \thechapter} % Display "Chapter X"
  {0.5em} % Space between "Chapter X" and the title
  {\Huge} % Chapter title format
\titlespacing*{\chapter}{0pt}{-20pt}{20pt} % Adjust spacing around chapter title

% ----------------------------------------------------------------



\begin{document}

\begin{titlepage}
  \centering
  \vspace*{2cm}
  {\LARGE\bfseries MPhil DIS Report 24\par}
  \vspace{1cm}
  {\Large\itshape\ CRSiD:\ tmb76\par}
  \vspace{1cm}
  {\Large\itshape\ University of Cambridge\par}
  \vfill
  {\large\today\par}
\end{titlepage}

\tableofcontents

\chapter{Executive Summary}


\chapter{Introduction}


One of the key steps of scientific method is reproducibility. Results must be reproducible by others, ensuring that the same conclusions can be drawn multiple times. If a result cannot be reproduced, it may be considered erroneous, or simply a random occurence. This project therefore has for a core aim to evaluate the reproducibility of the results of the Callaham et al. (2021)\cite{callaham2021learning} paper and to test the robustness of their method.

\vspace{5mm}

For many problems in engineering and physical sciences, equations involve a large number of terms and complex differential equations. Simulating them can be computationally expensive or unnecessarily so, due to multiple asymptotic local behaviours where the system is dominated by a subset of the terms. In such cases, one can simplify the equations to a balance between these dominant terms, and simulate the system with sufficient accuracy and relatively lower computational cost \cite{charney1990scale}. This method, known as dominant balance or scale analysis, has been a powerful tool in physics.

\vspace{5mm}

And though extremely useful, dominant balance also requires expertise and is usually done by hand in time-consuming proofs. This report discusses and verifies a novel approach, developped by Callaham et al. (2021)\cite{callaham2021learning}, which explores using data from a physical system and machine learning methods to identify dominant balances algorithmically. First, this report will focus on the paper and the research surrounding it. Delving into what the rationale behind the method is, and how it performed on a series of case studies, as well as verifying it through reproducing the results with alternative code. This will be done primarily focusing on one of the case studies, but also for most of the others. Additionally, other algorithms than the method's chosen one are used to test the robustness of the method. Second, the method will be used on a new dataset, from simulations of elasto-inertial turbulence, a property of polymer laden flow.


\chapter{Background}


As aforementionned, dominant balance or scale analysis is a powerful tool in simplifying the modelling of physical processes. Importantly, it helps better understand the physics at play in a system by identifying the subset of terms that truly matter in an equation for a given asymptotic case. Simplifying the equations also leads to easier computations by avoiding unnecessary complications of the model. Taking the example of meteorology, where modelling the entire atmosphere from the full Navier-Stokes equations of motion would have an immense computational cost. And a large part of improvements in numerical weather predictions can be attributed to scale analysis \cite{charney1947dynamics, phillips1963geostrophic, burger1958scale, yano2009scale}.

\vspace{5mm}

However, this process can be slow as it requires considerable expertise from researchers. And for most of the well studied physical systems, this was done by hand a few decades ago. But with the wealth of computational power and data science techniques there is today, an attempt at automating dominant balance can be made. First is the Portwood et al. (2016)\cite{portwood2016robust} paper which used a cumulative distribution function on local density gradients to separate each region of a stratified turbulent flow. But the method used is highly tailored for its case study, as the gradient of one of the terms is used, knowing it has dynamics discerning qualities. Further, the results are interpreted through expert analysis of the identified regions. Second is the work carried out in Lee \& Zaki (2018)\cite{lee2018detection} where an algorithm to detect different dynamical regions is introduced. But again it is through the use of case-specific variables (vorticity), which restrict the use of this algorithm to certain flows. Finally, Sonnewald et al. (2019)\cite{sonnewald2019unsupervised} used a K-Means clustering algorithm to identify dynamically distinct regions in the ocean. And though they do introduce the concept of using the terms in the governing equations as features, the identification of active terms is done through comparison of the magnitudes of each term in the equation. In other words, identifciation of dominant terms is not done algorithmically but ``manually''. Thus, these methods are mostly designed for specific case studies and partly rely on expert knowledge to interpret the results.

\vspace{5mm}

A similar challenge in data science and machine learning has been to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009)\cite{schmidt2009distilling} contributed to a breakthrough using symbolic regression to find linear and non-linear differential equations. And this was improved in Brunton et al. (2016)\cite{brunton2016discovering}. As symbolic regression is expensive, the problem was approached with sparse regression, which for high-dimensional problems means identifying a sparse governing equation. The rationale behind this is that governing equations usually having only a subset of terms being important, as in dominant balance. Lejarza \& Baldea (2022) further advanced this by using multiple basis functions and a non-linear moving horizon optimization to learn governing equations from noisy data. Deep learning methods have also been used in this effort. First, where the lagrangians are learned, therefore learning how to model complex physical systems, and learning symmetries and conservation laws, where other networks failed \cite{cranmer2020discovering}. Second, deep learning (Graph Neural Network) and symbolic regression are combined to create a general framework to recover equations of physical systems \cite{cranmer2020lagrangian}. This method has the advantage of being generalisable and therefore useable to extract plausible governing equations for unknown systems.

\vspace{5mm}

This generalisable quality is precisely the gap that Callaham et al. (2021)\cite{callaham2021learning} attempt to fill in the identification of dominant balance models. They propose a novel approach to take in simulated or measured data from a physical system, and extract dominant balance models with minimal user input. This means one could use it in conjunction with the above governing equation identifying methods, and essentially automatically learn the governing equations and asymptotic regimes of that equation for any given physical system. This is a very powerful result however, as Schmidt \& Lipson (2009) noted for their work, this method should be seen as a guiding tool to help indicate where scientists should focus their attention, rather than a definitive answer. This is an important point which will be further discussed in this report.


\chapter{Methodology}

The method proposed by Callaham et al. (2021)\cite{callaham2021learning} can be summarized in three steps.First, representing the data in an equation space, with the terms as features. So that, second, the Gaussian Mixture Model clustering algorithm can be used to identify groups with a similar balance of terms. And finally, using Sparce Principal Component Analysis to identify the active terms in each cluster.

\section{Data \& Equation Space Reprensentation}

As previously mentionned, obtaining the data can be done through simulations or measurements, with almost no restrictions as to what equations can be studied. The data comes in the form of space-time fields of physical variables: $u(\vec{x}, t)$, where $u$ is the physical variable considered (e.g. $\vec{u}$, $p$, etc. for the Navier-Stokes equations). The variables must be sufficient so that one may derive the terms of the equation from them. One important requirement is that the computed must all balance out to 0 for each point in time and space. And this comes down to computing the terms as they are in the equation, since this results in a linear covariance structure, as each term is balanced by a linear combination of the other terms\cite[Supplementary Information]{callaham2021learning}.

% INCLUDE: flow chart map of getting the terms from the data?

The main idea of the method proposed here is to reorganize these physical space fields into an equation space where each coordinate is one of the term in the equation, and each sample is a point in space and time. Thus each sample will be a vector $\vec{f} \in \mathbb{R}^k$ where $k$ is the number of terms in the equation, such that:

\begin{equation}
    \vec{f} = \begin{bmatrix} f_1(u(\vec{x}, t), \hdots) \\ f_2(u(\vec{x}, t), \hdots) \\ \vdots \\ f_k(u(\vec{x}, t), \hdots) \end{bmatrix}
\end{equation}

Where $f_i$ is the $i^{th}$ term in the equation, itself a function of the physical variables. Again, one must make sure that for each sample, the terms all balance out to 0.


\section{Gaussian Mixture Model Clustering}

Geometrically, by clustering points in this equation space, one can identify groups of points which have a similar balance of terms. Here the algorithm chosen is the Gaussian Mixture Model (GMM) clustering algorithm. This algorithm relies on the assumption that the data has been generated from a mixture of a finite number of Gaussian distributions with unknown parameters\cite{mit2015algorithmic}. It has the advantage of being able to identify clusters of with varying shapes and sizes. It only requires one hyperparameter which is the number of clusters one wants the algorithm to find, and therefore how many gaussian distributions the data is assumed to have been generated from\cite{sklearnGMM}.

The way GMMs fit to the data is by using the Expectation-Maximisation algorithm. This algorithm starts from an initial guess for the parameters of the gaussian distributions, the iteratively evaluates the posterior probability of each point belonging to each cluster, and updates the parameters with weighted averages\cite{dempster1977maximum}. This converges to the maximum likelihood estimates of the parameters. The algorithm is explained in greater detail in Algorithm 1 in the Appendix.

Another key output of the GMM is that its probabilistic nature allows for the identification of covariances between the terms in each cluster, as these are defined by a gaussian distribution and its covariance matrix.


\section{Sparse Principal Component Analysis}

With the points grouped in clusters of similar balance of terms, the next step is to identify which are active and which can be dropped to simply the equation. This is done through Sparse Principal Component Analysis (SPCA). Principal Component Analysis (PCA) is a method that is usually used to reduce the dimensionality of the data, whilst maximising the information kept. The new dimensions onto which the data is projected are called principal components and are the directions in which the data has the greatest variance\cite{lever2017principal}. SPCA differs from PCA in that it adds a sparsity constraint to the principal components, resulting in most coefficients of the PCs being zero. This is done by adding a L1 penalty to the objective function of PCA, which leads to some coefficients being shrunk down to 0\cite{zou2006sparse}. Again, SPCA is explained in greater detail in Algorithm 2 in the Appendix.


\chapter{Conducted research}

To test and validate this method Callaham et al. used a series of case studies\cite{callaham2021learning}. While some were well studied physical systems whose dominant balance regimes were well known (Turbulent boundary layer, Geostrophic balance currents), others were more complex and less understood (Optical pulse generation, rotating detonation engine). For these latter cases, the paper therefore presents a first plausible identification of dominant balance regimes. In this chapter, the aim is to discuss the reproducibility of the results of the paper, as well as testing and discussing the drawbacks of the method.

\section{Portability of the code}

One great quality of the Callaham et al. (2021)\cite{callaham2021learning} is the sharing of their code in the form of runnable notebooks. With the overarching motivation for this project being the importance of reproducibility in todays code-rich research environment, it is key that scientists share their code, so one may verify how the results were obtained. This is also a great asset as it allows for a more useful reproduction of the results, where explicitely different results can be tested.

Overall, the code is of great quality, though some portability issues were encountered. Some of the dependencies were not stated in their README. More importantly, it seemed some of the data generating code was modified between running their final notebook versions and their uploading to the repository. This lead to some troubleshooting to get the data to be the same as the one used in the paper. And for the flow past a cylinder case, a file needed in the setup of the Nek5000 simulation software was missing\cite{nek5000setup}. For the bursitng neuron case, the time for which the data was generated was wrongly set. Similarly for the gesotrophic current data, the remote reading code was selecting snapshots of the data that did not match the ones used in the paper, and the first 45 were set to zero, which had no real use. Further, the results in the paper are actually obtained from a different snapshot than the one stated in the paper. For this report, the data was simply downloaded from the HYCOM database \cite{hycom} and the time was set to the same as the one used in the paper.

Nevertheless, the code was otherwise easy to run and the notebook format was very useful to follow the logic of the code. From this, the code was first written for the turbulent boundary layer case. And explicitely alternative code was then written to test the reproducibility of the results.

\section{Reproducing of the results}

Reiterating previously made points, one of the main goals of this project is to verify the results of Callaham et al. (2021)\cite{callaham2021learning}. Not only check that the results can be reproduced but to do so using alternative code. The aim here is to have code differently written which in practice does the same thing. This is to make sure none of the main results from Callaham et al. are due to a "lucky" bug in their code.

\subsection{Turbulent Boundary Layer}

Because the underlying method is essentially the same for all case studies. The choice was made to put particular focus on just one of the cases, and less so on the others. The scenario chosen was the boundary layer in transition to turbulence.

Throughout the Callaham et al. (2021)\cite{callaham2021learning} Notebooks, aside from the 3 main steps of the method, the large majority of the code is the handling of the data, switching from equation space to physical space, and getting the unique balance models. Most of it is written using \texttt{Numpy}, which is the most efficient way. In the alternative code however, \texttt{Pandas} was mostly used. In terms of performance, it is close to \texttt{Numpy}, especially for the size of the data. For some code sections, however, some impractical work arounds were required, particularly to replace the \texttt{numpy.unique} method.

The data for this case study is obtained from the John Hopkins database, which ran a direct numerical simulation of a boundary layer over a stationary plate\cite{jhtdb}. In this database are given the x and y coordinates as well as the following variables: $u$, the streamwise component of velocity, $v$, the wall-normal component of velocity, $p$, the fluid pressure, and $(\overline{u' v'})$ \& $(\overline{u'^2})$, the wall-normal and streamwise Reynolds' stress term. From these variables, the terms of the streamwise component of the Reynolds Averaged Navier-Stokes Equation need to be calculated:

\begin{equation}
  \bar{u} \bar{u}_x + \bar{v} \bar{u}_y = \rho^{-1} \bar{p}_x + \nu \nabla^2 \bar{u}  - (\overline{u' v'})_y - (\overline{u'^2})_x
\end{equation}

In order to get the derivative terms, the method employed by Callaham et al. makes use of \texttt{scipy.sparse}'s sparse matrices. These sparse matrices are built so that when computing the matrix product with the fields or variables, the 2nd order forward/central/backward difference derivative following the method described in 'Fundamentals of Numerical Computations'\cite{finitediff}. Though it has the advantage of being portable for all variables, it is much slower than using the \texttt{numpy.gradient} function which does the same computation but more explicitely, when setting the \texttt{edge\_order} argument to 2. With the derivatives computed, space fields of each term in the equation can be plotted (see Fig. \ref{fig:RANS_terms})

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using the original Callaham et al. code}
  \label{fig:RANS_terms}
\end{figure}

The next step is to use a Gaussian Mixture Model to cluster the data in equation space. Quite judiciously, Callaham et al. chose to use the \texttt{sklearn} library's implementation of the GMM algorithm. It is likely a very efficient implementation of the algorithm, being written and optimised by experienced developpers. With the arguments chosen, the algorithm is initialised usimg the \texttt{'k-means++'} method. This method initialises the means of the gaussians using the centroids found by the K-Means clustering algorithm (see Algorithm 3), itself initialised based on the empirical probability distributions of the data\cite{arthur2007kmeans}. The covariance matrices are then initialised as the covariance matrices of the clusters found by the K-Means algorithm. The algorithm then uses the Expectation-Maximisation algorithm as expected. Convergence is evaluated using the log-probability of the data, and the algorithm stops when the difference between the new and old log-likelihood is less than $10^{-3}$.

The alternative code written follows the same initialisation, explicitely using \texttt{sklearn}'s K-Means algorithm, with \texttt{'k-means++'} initialisation. The Expectation-Maximisation algorithm is then implemented as described in Algorithm 1, using the total log-likelihood difference as the convergence criterion.

For Sparse principal component analysis, the \texttt{sklearn} library's implementation is used. The method used is based on an extension of space PCA, which makes use of structured regularization to constrain the sparsity patterns

With SPCA done, the active terms can be identified and from here, the original and alternative code only differ in whihc library they predominantly use. The original code uses mostly \texttt{Numpy}, whilst the alternative code uses \texttt{Pandas}, which sometimes had similar functions and sometimes needed a work around.


\subsection{Results}


The first result to check is the terms obtained in Fig. \ref{fig:RANS_terms}. These were obtained with the original code, which made use of \texttt{scipy.sparse}. The alternative code, which used \texttt{numpy.gradient} instead, gave the same results (see Fig. \ref{fig:custom_RANS_terms}). The reason why Callaham et al. used \texttt{scipy.sparse} is likely because it is more generalisable to all variables, however, they also used \texttt{numpy.gradient} for other case studies. And using \texttt{numpy.gradient} leads to considerable speed ups.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/BL/custom_RANS_terms.png}
  \caption{Plot of the 6 terms in the RANS equation (Eq 5.1), using an alternative code}
  \label{fig:custom_RANS_terms}
\end{figure}

The next step is to cluster the data in equation space. The resulting clusters' covariance matrices were obtained and are plotted in Figure \ref{fig:custom_GMM_cov_mat}. And plotting the current clustering in physical space gives the results in Figure \ref{fig:GMM_clusters}. As can be seen results do differ, with the alternative code needing 7 clusters to identify the transitional layer (instead of 6 for the original code). The difference could be explained by either the different convergence criterion or some of the covariance matrices regularisation that is done in the \texttt{sklearn} implementation\cite{sklearnGMM}.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_cov_mat.png}
      \caption{}
  \end{subfigure}
  \caption{Covariance matrices of for each of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm}
  \label{fig:custom_GMM_cov_mat}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.7\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_clustering_space.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the clusters found by the \texttt{sklearn} (a) and custom (b) GMM algorithm}
  \label{fig:GMM_clusters}
\end{figure}

Then comes applying SPCA to the points in each cluster to identify which terms are active. In this case, the \texttt{sklearn} function is still used, though with alternative code to handle the results. Due to the different clustering obtained, the optimal alpha values had to be different for each method. For the original code, the optimal alpha value was set to 10, but had to be set to 7 for the alternative code, in order to keep the


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_spca_residuals.png}
      \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_spca_residuals.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the residuals of the SPCA algorithm, when using the original Callaham et al. code (a) and an alternative code (b)}
  \label{fig:SPCA_residuals}
\end{figure}


One can see the active for the 2 codes in Fig. \ref{fig:active_terms}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/GMM_active_terms.png}
      \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../Plots/BL/custom_GMM_active_terms.png}
      \caption{}
  \end{subfigure}
  \caption{Plot of the terms identified as active by the SPCA algorithm, when using the original Callaham et al. code (a) and an alternative code (b)}
  \label{fig:active_terms}
\end{figure}


\section{Exploration of other algorithms}

\subsection{Spectral clustering}

\subsection{K-Means}

\subsection{Weighted K-Means}

\section{Stability Assessment}

\subsection{Under different number of clusters set}

\subsection{Under different training set size}

\chapter{Elasto-inertial turbulence}

\section{Background}

\section{Methodology}

\section{Results}

\section{Discussion}

\chapter{Conclusion}


\chapter{Appendix}

\section{Algorithms}


\begin{definitionbox}{Algorithm 1: Gaussian Mixture Model clustering}
  \begin{algorithmic}[1]
    \State Data $\vec{x} \in \mathbb{R}^{n \times n_{features}}$, number of clusters/Gaussians to fit $K$ \Comment{Inputs}
    \State Cluster assignments $z \in \mathbb{R}^n$ \Comment{Output}
    \State $w_{k} \gets \frac{1}{k}$ \Comment{Initialisation of the weights}
    \State $\mathbf{\mu_{k}} \gets \mathbf{c_{k}}$ \Comment{Initialise the means as K-Means centroids}
    \State $\mathbf{\Sigma_{k}} \gets \mathbf{\Sigma}_{C_{k}}$ \Comment{Initialise as the covariance matrices of the clusters}
    \State $C_{k} \gets w_{k} \times \mathcal{N}(\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ \Comment{Initialise the clusters/Gaussians}

    \State \textbf{Expectation-Maximisation:}
    \State $\mathcal{L} \gets 0$ \Comment{Initialise the log-likelihood}
    \For{$i < \text{max-iter}$}
      \State \textbf{Expectation step:}
      \For{$k \in K$}
        \State $\mathbf{b_{k}} \gets \frac{w_{k} \times f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})}{\sum_{j=1}^{K} w_{j} \times \mathcal{N}(\mathbf{x}|\mathbf{\mu_{j}}, \mathbf{\Sigma_{j}})}$ \Comment{Compute the responsibilities for each cluster}
      \EndFor
      \State \textbf{Maximisation step:}
      \For{$k \in K$}
        \State $w_{k} \gets \frac{1}{n} \sum_{i=1}^{n} \mathbf{b}_{k}$ \Comment{Update the weights}
        \State $\mathbf{\mu_{k}} \gets (\sum \mathbf{b}_{k}\mathbf{x})/\sum \mathbf{b}_{k}$ \Comment{Update the means}
        \State $\mathbf{\Sigma_{k}} \gets (\sum \mathbf{b}_{k}(\mathbf{x} - \mathbf{\mu}_{k})^{2})/\sum \mathbf{b}_{k}$ \Comment{Update the covariance matrices}
      \EndFor
      \State $\mathcal{L}_{\text{new}} \gets \sum_{i=1}^{n} \log(\sum_{k=1}^{K} w_{k} \times f(x_{i}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}}))$ \Comment{Compute the log-likelihood}
      \State $\epsilon \gets \mathcal{L} - \mathcal{L}_{\text{new}}$ \Comment{Compute the difference in log-likelihood}
      \If{$\epsilon < 10^{-4}$}
        \State \textbf{break}
      \EndIf
      \State $\mathcal{L} \gets \mathcal{L}_{\text{new}}$ \Comment{Update the log-likelihood}
    \EndFor


  \end{algorithmic}
  Where:

  - $f(\mathbf{x}|\mathbf{\mu_{k}}, \mathbf{\Sigma_{k}})$ is the probability density function of a multivariate normal distribution
\end{definitionbox}



\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
