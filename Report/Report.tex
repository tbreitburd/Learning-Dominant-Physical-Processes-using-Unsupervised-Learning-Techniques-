\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{setspace} % Add the setspace package


% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}
% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}[1]{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=#1,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}[1]{
  colback=green!5!white,    % Light green background
  colframe=green!75!black,   % Darker green frame
  colbacktitle=green!85!black,  % Even darker green for the title background
  title=#1,         % Title text for example box
  fonttitle=\bfseries,    % Bold title font
  enhanced,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}


\onehalfspacing

\geometry{top=1.5in} % Adjust the value as needed
% ----------------------------------------------------------------



\begin{document}

\begin{titlepage}
  \centering
  \vspace*{2cm}
  {\LARGE\bfseries MPhil DIS Report 24\par}
  \vspace{1cm}
  {\Large\itshape\ CRSiD:\ tmb76\par}
  \vspace{1cm}
  {\Large\itshape\ University of Cambridge\par}
  \vfill
  {\large\today\par}
\end{titlepage}

\tableofcontents

\chapter{Executive Summary}


\chapter{Introduction}


One of the key steps of scientific method is reproducibility. Results must be reproducible by others, ensuring that the same conclusions can be drawn multiple times. If a result cannot be reproduced, it may be considered erroneous, or simply a random occurence. An important aspect of this project will be to evaluate the reproducibility of the results of Callaham et al. (2021)\cite{callaham2021learning} and to test the robustness of their method.

For many problems in engineering and physical sciences, equations involve a large number of terms and complex differential equations. Simulating them can be computationally expensive or unnecessarily so, due to multiple asymptotic local behaviours where the system is dominated by a subset of the terms. In such cases, one can simplify the equations to a balance between these dominant terms, and simulate the system with sufficient accuracy and relatively lower computational cost (REFERENCE FOR THIS). This method, known as dominant balance or scale analysis, has been a powerful tool in physics.

And though extremely useful, dominant balance also requires expertise and is usually done by hand in time-consuming proofs. This report discusses and verifies a novel approach, developped by Callaham et al. (2021)\cite{callaham2021learning}, an applies it to new data. First, this report will focus on the paper and the research surrounding it. Delving into what the rationale behind the method is, and how it performed on a series of case studies, as well as verifying it through reproducing the results with alternative code. This will be done primarily focusing on one of the case studies, but also for most of the others. Additionally, other algorithms than the method's chosen one are used to test the robustness of the method. Second, the method will be used on a new dataset, from simulations of elasto-inertial turbulence, a property of of polymer laden flow.


\chapter{Background}



Dominant balance is a powerful tool in simplifying the modelling of physical processes. Importantly, it helps better understand the physics at play in a system by identifying the subset of terms that matter in an equation for a specific asymptotic case. This understanding of interactions between terms and the physics they represent can be illustrated through examples in multiple fields of physics.

Taking the example of:



There has been some research to automate the process of finding the dominant balance in a system. First is the Portwood et al. (2016)\cite{portwood2016robust} paper which uses a cumulative distribution function on the local density gradient to separate each region. THis method is then highly tailored to this case, where a gradient of one of the terms is used, knowing it can help discern dynamically distinct regions, and then interprets many results from expert analysis of the identified regions. Second is the Lee \& Zaki (2018)\cite{lee2018detection} which introduces an algorithm to detect different dynamical regions, but again through the use of case-specific varaibles (vorticity), which restrict the use of this algorithm to specific flows. Finally, the Sonnewald et al. (2019)\cite{sonnewald2019unsupervised} paper which uses a K-Means clustering algorithm to identify different regions in the ocean. And they do introduce the concept of using the terms in the governing equations as features, but identification of active terms is done through comparison of the magnitudes of each term in the equation. In other words, identifciation of dominant terms is not done algorithmically but ``manually''.
They either focused on a specific application and used expert-knowledge on it or only clustered the data and then relied on expert knowledge to determine which terms were active in that cluster.

``Automating the process of finding the dominant balance in a system has been the focus of several studies. For instance, Portwood et al. (2016) use a cumulative distribution function on the local density gradient to separate regions dynamically, tailored to their specific case. Lee \& Zaki (2018) introduce an algorithm to detect different dynamical regions using vorticity, but this is also specific to certain flows. Sonnewald et al. (2019) use a K-Means clustering algorithm to identify different regions in the ocean, employing terms in the governing equations as features. However, the identification of dominant terms is done manually by comparing magnitudes.''



A similar interesting data science and machien learning challenge has been to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009)\cite{schmidt2009distilling} contributed to a breakthrough using symbolic regression to find linear and non-linear differential equations. Brunton et al. (2016)\cite{brunton2016discovering} is another example of this, improving on Schmidt \& Lipson's (2009)\cite{schmidt2009distilling} work. As symbolic regression is expensive, the problem was now approahced with sparse regression, which for high-diemensioanl problems means identifying a sparse governing equation and this makes use of the governing equations usually having only a subset of terms being important. In Lejarza \& Baldea (2022)\cite{lejarza2022data}, the governing equations are learned from noisy data using multiple basis functions, and a non-linear moving horizon optimization. Though it can only handle ODEs, it provides an approach for thresholding hte basis functions that result in the found governing equation are of lower complexity than those obtained thorugh sparse regression based approches.

``A similar challenge in data science and machine learning is to directly find the laws and equations that govern a system from data. Schmidt \& Lipson (2009) used symbolic regression to find differential equations, followed by Brunton et al. (2016) who improved on this work using sparse regression. This approach is efficient for high-dimensional problems, as it identifies a sparse governing equation where only a subset of terms are important. Lejarza \& Baldea (2022) further advanced this by using multiple basis functions and a non-linear moving horizon optimization to learn governing equations from noisy data''



Much like in more recent work where neural networks are used to determine hte physics at play from data \cite{cranmer2020discovering}\cite{cranmer2020lagrangian}.

The method developped by Callaham et al. (2021)\cite{callaham2021learning} is a novel approach to finding the dominant balance in a system. And it could be used in conjunction with the above governing equation identifying methods. But just like Schmidt \& Lipson (2009)\cite{schmidt2009distilling} points out for their own work, this method should not be considered as a perfect tool to find all the dominant physical processes in a certin case study but rather as a guiding tool to help indicate where scientists should focus their attention.
``The method developed by Callaham et al. (2021) is a novel approach to finding the dominant balance in a system. It could be used alongside other methods for identifying governing equations. However, as Schmidt \& Lipson (2009) noted for their work, this method should be seen as a guiding tool to help indicate where scientists should focus their attention, rather than a perfect tool for finding all dominant physical processes in a given case study.''





\chapter{Methodology}


Get simulated data of the terms in the equation of physical variables from which terms in the equation can be derived

Group the data into feature space, with each term as a feature

Cluster the data using GMM

SPCA to identify which terms are active in each cluster

Group together clusters that have the same active terms


\chapter{Conducted research}

Callaham has used the algorithm in a few different cases to validate how it functions.



\section{Portability of the code}

\section{Reproducibility of the results}

\section{Exploration of other algorithms}

\subsection{Spectral clustering}

\subsection{K-Means}

\subsection{Weighted K-Means}

\section{Stability Assessment}

\subsection{Under different number of clusters set}

\subsection{Under different training set size}

\chapter{Elasto-inertial turbulence}

\section{Background}

\section{Methodology}

\section{Results}

\section{Discussion}

\chapter{Data analysis pipeline}


\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
